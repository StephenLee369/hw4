{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-714 Homework 4\n",
    "\n",
    "In this homework, you will leverage all of the components built in the last three homeworks to solve some modern problems with high performing network structures. We will start by adding a few new ops leveraging our new CPU/CUDA backends. Then, you will implement convolution, and a convolutional neural network to train a classifier on the CIFAR-10 image classification dataset. Then, you will implement recurrent and long-short term memory (LSTM) neural networks, and do word-level prediction language modeling on the Penn Treebank dataset. \n",
    "\n",
    "As always, we will start by copying this notebook and getting the starting code.\n",
    "Reminder: __you must save a copy in drive__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to set up the assignment\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/\n",
    "!mkdir -p 10714\n",
    "%cd /content/drive/MyDrive/10714\n",
    "!git clone https://github.com/dlsys10714/hw4.git\n",
    "%cd /content/drive/MyDrive/10714/hw4\n",
    "\n",
    "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "!pip3 install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=./python\n",
      "env: NEEDLE_BACKEND=nd\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH ./python\n",
    "%set_env NEEDLE_BACKEND nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar-10-batches-py/\n",
      "cifar-10-batches-py/data_batch_4\n",
      "cifar-10-batches-py/readme.html\n",
      "cifar-10-batches-py/test_batch\n",
      "cifar-10-batches-py/data_batch_3\n",
      "cifar-10-batches-py/batches.meta\n",
      "cifar-10-batches-py/data_batch_2\n",
      "cifar-10-batches-py/data_batch_5\n",
      "cifar-10-batches-py/data_batch_1\n"
     ]
    }
   ],
   "source": [
    "# Download the datasets you will be using for this assignment\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n",
    "    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish setting up the assignment, go ahead and fill in all the code in `python/needle/autograd.py` using your solution code from the previous homework. Also copy the solutions in `src/ndarray_backend_cpu.cc` and `src/ndarray_backend_cuda.cu` from homework 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ND Backend [10 pts]\n",
    "\n",
    "Recall that in homework 2, the `array_api` was imported as `numpy`. In this part, the goal is to write the necessary operations with `array_api` imported from the needle backend `NDArray` in `python/needle/backend_ndarray/ndarray.py`. Make sure to copy the solutions for `reshape`, `permute`, `broadcast_to` and `__getitem__` from homework 3.\n",
    "\n",
    "Fill in the following classes in `python/needle/ops_logarithmic.py` and `python/needle/ops_mathematic.py`:\n",
    "\n",
    "- `PowerScalar`\n",
    "- `EWiseDiv`\n",
    "- `DivScalar`\n",
    "- `Transpose`\n",
    "- `Reshape`\n",
    "- `BroadcastTo`\n",
    "- `Summation`\n",
    "- `MatMul`\n",
    "- `Negate`\n",
    "- `Log`\n",
    "- `Exp`\n",
    "- `ReLU`\n",
    "- `LogSumExp`\n",
    "- `Tanh` (new)\n",
    "- `Stack` (new)\n",
    "- `Split` (new)\n",
    "\n",
    "Note that for most of these, you already wrote the solutions in the previous homework and you should not change most part of your previous solution, if issues arise, please check if the `array_api` function used is supported in the needle backend. \n",
    "\n",
    "`TanhOp`, `Stack`, and `Split` are newly added. `Stack` concatenates same-sized tensors along a new axis, and `Split` undoes this operation. The gradients of the two operations can be written in terms of each other. We do not directly test `Split`, and only test the backward pass of `Stack` (for which we assume you used `Split`).\n",
    "\n",
    "**Note:** You may want to make your Summation op support sums over multiple axes; you will likely need it for the backward pass of the BroadcastTo op if yours supports broadcasting over multiple axes at a time. However, this is more about ease of use than necessity, and we leave this decision up to you (there are no corresponding tests).\n",
    "\n",
    "**Note:** Depending on your implementations, you may want to ensure that you call `.compact()` before reshaping arrays. (If this is necessary, you will run into corresponding error messages later in the assignment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.2, pluggy-1.5.0 -- /home/ljq/anaconda3/envs/taichi/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/ljq/gpu/CMU/hw4\n",
      "collected 1803 items / 1685 deselected / 118 selected                          \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m    [  0%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m  [  1%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m    [  2%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m  [  3%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m   [  4%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m   [  5%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m   [  7%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m   [  9%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 11%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 12%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 14%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 15%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 16%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 16%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 17%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 18%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 19%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 20%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 21%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 22%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 22%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 23%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 24%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 25%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 26%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 27%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 27%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 28%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 29%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 30%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 31%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 32%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 33%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 33%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 34%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 35%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                [ 36%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                [ 37%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 38%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 38%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                [ 39%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                [ 40%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 41%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 42%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 43%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 44%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 44%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 45%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 46%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 47%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 48%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 49%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m      [ 50%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m      [ 50%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 51%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 52%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cpu-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 53%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cpu-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 54%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cpu-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 55%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cuda-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 55%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cuda-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 56%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cuda-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 57%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[31m     [ 63%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 64%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 65%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 66%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[31m    [ 66%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 67%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 68%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 69%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 72%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 72%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m  [ 80%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m  [ 81%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes0-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 83%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes0-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 84%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes1-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 85%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes1-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 86%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-None-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 87%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-None-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 88%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes0-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 88%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes0-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 89%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes1-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 90%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes1-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 91%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-None-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 92%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-None-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 93%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 94%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 94%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 95%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 96%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 97%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 98%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 99%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m       [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m____________________________ test_relu[cpu-shape0] _____________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_relu\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.maximum(_A, \u001b[94m0\u001b[39;49;00m), ndl.relu(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.62864095]]])\n",
      "_A         = array([[[-0.62864095]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:126: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f86afa54af0>, array([[[0.]]], dtype=float32), array([[[-0.62864095]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference: 0.62864095\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference: 1.\u001b[0m\n",
      "\u001b[1m\u001b[31mE            x: array([[[0.]]], dtype=float32)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            y: array([[[-0.628641]]], dtype=float32)\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f86afa54af0>, array([[[0.]]], dtype=float32), array([[[-0.62864095]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f86afa577f0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f86af5b0250>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________________________ test_relu[cpu-shape1] _____________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_relu\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.maximum(_A, \u001b[94m0\u001b[39;49;00m), ndl.relu(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-2.95693666e-01 -2.02359939e+00  9.32137251e-01  7.92260528e-01\n",
      "    3.07000041e-01 -7.93553650e-01]\n",
      " ...37237036e-01]\n",
      "  [ 1.21563613e+00 -1.16565275e+00 -6.06325090e-01 -1.10850406e+00\n",
      "   -4.95121866e-01 -1.55885565e+00]]])\n",
      "_A         = array([[[-2.95693666e-01, -2.02359939e+00,  9.32137251e-01,\n",
      "          7.92260528e-01,  3.07000041e-01, -7.93553650e-01..., -1.16565275e+00, -6.06325090e-01,\n",
      "         -1.10850406e+00, -4.95121866e-01, -1.55885565e+00]]],\n",
      "      dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:126: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f86af5d2c20>, array([[[0.        , 0.        , 0.93213725, 0.7922605...3 ],\n",
      "        [ 1.2156361 ,  0.4537548 ,  1.3760866 ,  1.1401178 ,\n",
      "          1.085602  ,  0.1408631 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           (shapes (4, 5, 6), (1, 5, 6) mismatch)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            x: array([[[0.      , 0.      , 0.932137, 0.792261, 0.307   , 0.      ],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [0.      , 0.243023, 0.091793, 0.      , 0.      , 0.      ],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [0.      , 0.      , 0.      , 1.467664, 0.065575, 0.725642],...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            y: array([[[ 1.0976  , -0.252545,  1.470051,  0.792261,  0.956658,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     1.379938],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 0.697125,  1.352135,  1.475442,  1.104438,  0.039942,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f86af5d2c20>, array([[[0.        , 0.        , 0.93213725, 0.7922605...3 ],\n",
      "        [ 1.2156361 ,  0.4537548 ,  1.3760866 ,  1.1401178 ,\n",
      "          1.085602  ,  0.1408631 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f86afa577f0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f86af5b0250>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________________________ test_relu[cuda-shape1] ____________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_relu\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.maximum(_A, \u001b[94m0\u001b[39;49;00m), ndl.relu(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.05377588  0.20338777  0.16131176  0.04317565 -1.4132142\n",
      "    0.14171205]\n",
      "  [-0.5501977  -1.1564685 ...0.14296173  0.12586237\n",
      "   -0.20687069]\n",
      "  [-0.79452735  0.3885921   0.13083026  0.47227657 -2.1078956\n",
      "    0.21374775]]])\n",
      "_A         = array([[[-0.05377588,  0.20338777,  0.16131176,  0.04317565,\n",
      "         -1.4132142 ,  0.14171205],\n",
      "        [-0.5501977 ,...069],\n",
      "        [-0.79452735,  0.3885921 ,  0.13083026,  0.47227657,\n",
      "         -2.1078956 ,  0.21374775]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:126: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f86af5d2ef0>, array([[[0.        , 0.20338777, 0.16131176, 0.0431756...8 ],\n",
      "        [ 0.8006239 ,  3.8168006 ,  0.24735339,  2.1113443 ,\n",
      "          1.5876245 ,  1.2074844 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           (shapes (4, 5, 6), (1, 5, 6) mismatch)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            x: array([[[0.      , 0.203388, 0.161312, 0.043176, 0.      , 0.141712],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [0.      , 0.      , 0.168161, 0.      , 1.794351, 0.      ],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [0.200031, 1.195579, 0.      , 0.      , 1.196289, 0.      ],...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            y: array([[[ 1.182686,  0.203388,  1.817985,  0.587814,  1.092556,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                     1.241576],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-0.052806,  0.1814  ,  1.121725,  1.799782,  1.794351,...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f86af5d2ef0>, array([[[0.        , 0.20338777, 0.16131176, 0.0431756...8 ],\n",
      "        [ 0.8006239 ,  3.8168006 ,  0.24735339,  2.1113443 ,\n",
      "          1.5876245 ,  1.2074844 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f86afa577f0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f86af5b0250>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cpu-shape0] _____________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'tanh'\u001b[0m\n",
      "\n",
      "A          = needle.Tensor([[[-1.333942]]])\n",
      "_A         = array([[[-1.333942]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cpu-shape1] _____________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'tanh'\u001b[0m\n",
      "\n",
      "A          = needle.Tensor([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      " ...50593936e-01]\n",
      "  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]])\n",
      "_A         = array([[[ 4.68197703e-01,  2.24348330e+00,  4.72604215e-01,\n",
      "          1.18212029e-01, -6.31235898e-01,  1.17569625e+00...,  1.98646748e+00, -1.20881248e+00,\n",
      "          8.64627600e-01,  1.01337186e-03, -9.01962698e-01]]],\n",
      "      dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cuda-shape0] ____________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'tanh'\u001b[0m\n",
      "\n",
      "A          = needle.Tensor([[[-2.3530958]]])\n",
      "_A         = array([[[-2.3530958]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cuda-shape1] ____________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'tanh'\u001b[0m\n",
      "\n",
      "A          = needle.Tensor([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.1732628...0.8592022   0.2965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]])\n",
      "_A         = array([[[-0.5291037 , -1.1316243 ,  1.2829843 ,  0.6779355 ,\n",
      "         -0.32166547, -0.51734763],\n",
      "        [ 0.48438653,...48 ],\n",
      "        [-0.8919766 , -0.5311779 ,  1.3061328 ,  0.70647854,\n",
      "          0.99664706, -1.344323  ]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:134: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_tanh_backward[cpu-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.tanh, A)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'tanh'\u001b[0m\n",
      "\n",
      "A          = needle.Tensor([[[-1.5997132]]])\n",
      "_A         = array([[[-1.5997132]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:142: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_tanh_backward[cpu-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.tanh, A)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'tanh'\u001b[0m\n",
      "\n",
      "A          = needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119   ... -0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]])\n",
      "_A         = array([[[ 1.6233783 , -0.58456963,  0.86539763,  0.8150671 ,\n",
      "          0.52470696, -2.4929152 ],\n",
      "        [ 0.28299394,...27 ],\n",
      "        [ 0.07837614, -0.39979827, -0.04655599, -1.3993185 ,\n",
      "          2.621195  , -0.13064201]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:142: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_tanh_backward[cuda-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.tanh, A)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'tanh'\u001b[0m\n",
      "\n",
      "A          = needle.Tensor([[[-1.2775043]]])\n",
      "_A         = array([[[-1.2775043]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:142: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_tanh_backward[cuda-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.tanh, A)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'tanh'\u001b[0m\n",
      "\n",
      "A          = needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334... 0.02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]])\n",
      "_A         = array([[[-0.71145284,  0.96028835, -0.01700081,  0.18537076,\n",
      "         -0.92995864,  1.2871389 ],\n",
      "        [-0.69594264,...502],\n",
      "        [-0.03949696, -0.9310424 , -0.55855966, -1.2472047 ,\n",
      "         -1.1149031 , -0.5235176 ]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:142: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape0-0-1] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'stack'\u001b[0m\n",
      "\n",
      "A          = [needle.Tensor([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.25607443 -0.30351138 -1...7868 -0.38599226  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]])]\n",
      "A_t        = [tensor([[ 0.9080,  1.1915,  0.6277,  1.7709,  2.5803],\n",
      "        [ 1.0994, -0.2561, -0.3035, -1.2755,  1.4340],\n",
      "       ....9323],\n",
      "        [-0.1727, -0.3860,  0.6065,  0.3832, -1.2306],\n",
      "        [-0.7605,  1.3836, -1.3960, -0.2341,  0.9361]])]\n",
      "_A         = [array([[ 0.90795034,  1.1914672 ,  0.6276671 ,  1.7709465 ,  2.5802875 ],\n",
      "       [ 1.0994289 , -0.25607443, -0.303511...8323304, -1.2305677 ],\n",
      "       [-0.76054263,  1.3835862 , -1.396009  , -0.23408563,  0.9360824 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape1-0-2] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'stack'\u001b[0m\n",
      "\n",
      "A          = [needle.Tensor([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727456   0.7769845  -0...071  -0.5883452  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]])]\n",
      "A_t        = [tensor([[ 0.2389,  1.7993,  0.1729, -0.1885,  1.6104],\n",
      "        [-0.5798, -1.7727,  0.7770, -0.6439,  0.3592],\n",
      "       ....3381],\n",
      "        [-1.3337, -0.5883, -1.3194, -2.0161,  0.7420],\n",
      "        [-1.2923,  0.1274, -1.2252,  1.4211,  1.0188]])]\n",
      "_A         = [array([[ 0.23885646,  1.7992946 ,  0.17290778, -0.18845753,  1.6103987 ],\n",
      "       [-0.57978904, -1.7727456 ,  0.776984...160983 ,  0.74200845],\n",
      "       [-1.2923242 ,  0.1273649 , -1.225224  ,  1.421074  ,  1.0188165 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape2-2-5] __________________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'stack'\u001b[0m\n",
      "\n",
      "A          = [needle.Tensor([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.18995644]\n",
      "  [-1.569928 ... 1.0998238  -0.07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]])]\n",
      "A_t        = [tensor([[[-1.3506, -1.0219, -0.2898, -0.5111, -1.0806, -0.1566, -0.1900],\n",
      "         [-1.5699,  0.1507, -1.5068,  0.479...2785,  0.1121, -0.2383,  1.0998, -0.0714],\n",
      "         [ 0.6218,  0.6492,  1.0329, -0.8242, -0.0909,  0.3844,  0.3634]]])]\n",
      "_A         = [array([[[-1.3505836 , -1.0219276 , -0.28979254, -0.5110697 ,\n",
      "         -1.0806233 , -0.156554  , -0.18995644],\n",
      "       ...[ 0.62180406,  0.64916927,  1.0328673 , -0.82423985,\n",
      "         -0.09091089,  0.3844313 ,  0.36341754]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cpu()\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape0-0-1] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'stack'\u001b[0m\n",
      "\n",
      "A          = [needle.Tensor([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653674  -1.0452379   1...948   0.6366247  -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]])]\n",
      "A_t        = [tensor([[-0.8576, -0.0482,  0.2440,  1.8527,  0.0825],\n",
      "        [-1.3493,  0.7654, -1.0452,  1.2154, -1.8788],\n",
      "       ....7364],\n",
      "        [ 0.6130,  0.6366, -0.1788,  1.4526,  0.0686],\n",
      "        [-0.0181, -0.1486, -1.0339, -1.0570, -0.2119]])]\n",
      "_A         = [array([[-0.85764205, -0.04821775,  0.24398687,  1.8527063 ,  0.08247776],\n",
      "       [-1.3493153 ,  0.7653674 , -1.045237...526075 ,  0.06863618],\n",
      "       [-0.0180711 , -0.14859328, -1.0338513 , -1.0570263 , -0.2118788 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape1-0-2] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'stack'\u001b[0m\n",
      "\n",
      "A          = [needle.Tensor([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843735  -0.6963952   0...7274  1.2440314  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]])]\n",
      "A_t        = [tensor([[-0.0361,  1.2204,  0.7632, -0.0202,  2.2113],\n",
      "        [-0.4971,  1.6844, -0.6964,  0.4664,  0.3781],\n",
      "       ....2980],\n",
      "        [ 0.0499,  1.2440, -1.3311,  0.1839, -0.7144],\n",
      "        [-1.1293, -1.0743,  0.6784,  0.4646,  1.6899]])]\n",
      "_A         = [array([[-0.03613054,  1.2204468 ,  0.7631518 , -0.02020102,  2.2112603 ],\n",
      "       [-0.49705917,  1.6843735 , -0.696395...8387789, -0.7143793 ],\n",
      "       [-1.1292638 , -1.0743438 ,  0.6784423 ,  0.46458447,  1.6899037 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape2-2-5] __________________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'stack'\u001b[0m\n",
      "\n",
      "A          = [needle.Tensor([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-01  5.57456922e-04  ...02]\n",
      "  [ 1.1806753e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]])]\n",
      "A_t        = [tensor([[[-5.5682e-01, -8.2674e-01, -9.5333e-01, -1.0309e-01, -4.2893e-01,\n",
      "           5.5746e-04,  1.0586e+00],\n",
      "     ...02],\n",
      "         [ 1.1807e+00, -5.7011e-01, -1.6425e-01,  1.0571e-01, -1.1592e-02,\n",
      "           6.3662e-02,  3.1759e-01]]])]\n",
      "_A         = [array([[[-5.56821287e-01, -8.26741934e-01, -9.53326285e-01,\n",
      "         -1.03087865e-01, -4.28928941e-01,  5.57456922e-0...011104e-01, -1.6424966e-01,  1.0570560e-01,\n",
      "         -1.1592215e-02,  6.3661553e-02,  3.1758705e-01]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cuda()\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape0-0-1] ______________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'stack'\u001b[0m\n",
      "\n",
      "A          = [needle.Tensor([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725866   0.82784927 -0...1785 -0.1998671  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]])]\n",
      "A_t        = [tensor([[-0.9027, -0.7916,  2.7447,  1.3618, -0.8795],\n",
      "        [-0.8366, -1.3726,  0.8278, -0.1998, -1.3305],\n",
      "       ...0961, -0.1999, -0.4484,  1.0540,  0.0305],\n",
      "        [-0.4769,  1.5635, -1.0746,  0.9731, -1.0777]], requires_grad=True)]\n",
      "_A         = [array([[-0.9026891 , -0.7916189 ,  2.7447178 ,  1.3617536 , -0.8795233 ],\n",
      "       [-0.8365751 , -1.3725866 ,  0.827849...540462 ,  0.03045906],\n",
      "       [-0.4768823 ,  1.5634812 , -1.0746397 ,  0.9731278 , -1.0777118 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "i          = 0\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape1-0-2] ______________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'stack'\u001b[0m\n",
      "\n",
      "A          = [needle.Tensor([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692809   1.112041   -0...7174  0.29785007  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]])]\n",
      "A_t        = [tensor([[-1.1209, -0.4109, -0.1834,  1.0602,  0.6528],\n",
      "        [ 0.5972,  0.9693,  1.1120, -0.5864,  0.0547],\n",
      "       ...3290,  0.2979,  0.3833,  0.1805,  0.1305],\n",
      "        [ 0.8502,  0.6713,  1.1949, -0.3069, -0.4117]], requires_grad=True)]\n",
      "_A         = [array([[-1.1208949 , -0.41087958, -0.18338212,  1.0601658 ,  0.6527857 ],\n",
      "       [ 0.5972073 ,  0.9692809 ,  1.112041...8051435,  0.1305337 ],\n",
      "       [ 0.8501626 ,  0.6712771 ,  1.1948769 , -0.30687174, -0.41171578]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "i          = 1\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape2-2-5] ______________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'stack'\u001b[0m\n",
      "\n",
      "A          = [needle.Tensor([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220489 ]\n",
      "  [ 0.2332015...-1.0654556  -0.26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]])]\n",
      "A_t        = [tensor([[[-0.2897,  0.5881,  1.2632,  1.8495, -1.3362, -0.2293,  1.6220],\n",
      "         [ 0.2332,  0.4612, -1.1698,  1.140...0655, -0.2649],\n",
      "         [-2.1368, -0.2813,  0.5420, -0.8280, -0.1952,  1.2885,  1.6259]]],\n",
      "       requires_grad=True)]\n",
      "_A         = [array([[[-0.28966966,  0.5881318 ,  1.2632097 ,  1.8494787 ,\n",
      "         -1.3362346 , -0.22933096,  1.6220489 ],\n",
      "       ...[-2.136809  , -0.28127787,  0.54197794, -0.8279596 ,\n",
      "         -0.19517688,  1.2884692 ,  1.625865  ]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cpu()\n",
      "i          = 4\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape0-0-1] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'stack'\u001b[0m\n",
      "\n",
      "A          = [needle.Tensor([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.11915252 -0.32940993  0...197  -0.6012362  -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]])]\n",
      "A_t        = [tensor([[-1.3193,  0.6103, -1.2339, -0.6026,  0.8272],\n",
      "        [ 1.3493,  0.1192, -0.3294,  0.6092,  1.7408],\n",
      "       ...5734, -0.6012, -0.2230,  0.0861,  0.1369],\n",
      "        [ 1.5095, -0.7127, -0.7338, -1.1747, -0.3218]], requires_grad=True)]\n",
      "_A         = [array([[-1.3193388 ,  0.6103235 , -1.2338529 , -0.6025919 ,  0.82718664],\n",
      "       [ 1.349268  ,  0.11915252, -0.329409...8610313,  0.13694113],\n",
      "       [ 1.5095038 , -0.7126971 , -0.73380625, -1.1747231 , -0.3217507 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "i          = 0\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape1-0-2] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'stack'\u001b[0m\n",
      "\n",
      "A          = [needle.Tensor([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.35942334  0.5834437  -2...63    0.94000864  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]])]\n",
      "A_t        = [tensor([[ 0.8628, -1.1587, -1.0618,  0.0683,  0.4480],\n",
      "        [-1.6560,  0.3594,  0.5834, -2.7582,  0.1567],\n",
      "       ...2584,  0.9400,  1.4049,  2.0662,  1.9755],\n",
      "        [ 0.7066, -1.1856, -1.0203, -0.1288, -0.1320]], requires_grad=True)]\n",
      "_A         = [array([[ 0.86278856, -1.1587424 , -1.0617669 ,  0.06834911,  0.44795617],\n",
      "       [-1.6559892 ,  0.35942334,  0.583443...662467 ,  1.975455  ],\n",
      "       [ 0.70656043, -1.1856185 , -1.0202792 , -0.12881304, -0.13199753]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "i          = 1\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape2-2-5] _____________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle' has no attribute 'stack'\u001b[0m\n",
      "\n",
      "A          = [needle.Tensor([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918304]\n",
      "  [ 0.5852232 ...    0.24931994 -1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]])]\n",
      "A_t        = [tensor([[[-0.5643,  0.2627, -0.2986,  2.1484,  2.2822,  2.0546, -0.5192],\n",
      "         [ 0.5852,  0.2980, -0.4843, -0.240...2493, -1.1226],\n",
      "         [ 1.3219, -0.1445, -0.9866,  0.2616,  0.9347, -0.3751,  0.2546]]],\n",
      "       requires_grad=True)]\n",
      "_A         = [array([[[-0.56430817,  0.26270753, -0.29860306,  2.1484196 ,\n",
      "          2.282201  ,  2.0546498 , -0.51918304],\n",
      "       ...[ 1.3219295 , -0.14445436, -0.9865747 ,  0.2616311 ,\n",
      "          0.93468   , -0.37505797,  0.25460657]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cuda()\n",
      "i          = 4\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_summation[cpu-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[ 0.7043908   1.2221886   1.5080935 ]\n",
      " [-0.585842    0.04551945  1.1883969 ]\n",
      " [ 0.35897374  0.82490486  0.5144381 ]\n",
      " [-0.14003062 -0.20621192 -0.23054962]\n",
      " [-2.1563048  -1.2538761   1.090643  ]])\n",
      "_A         = array([[ 0.7043908 ,  1.2221886 ,  1.5080935 ],\n",
      "       [-0.585842  ,  0.04551945,  1.1883969 ],\n",
      "       [ 0.35897374,  ...4381 ],\n",
      "       [-0.14003062, -0.20621192, -0.23054962],\n",
      "       [-2.1563048 , -1.2538761 ,  1.090643  ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cpu()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f86af5d3f40>, array([-1.8188128 ,  0.63252497,  4.071022  ], dtype=float32), array([[-1.8188128 ,  0.63252497,  4.071022  ]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           (shapes (3,), (1, 3) mismatch)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            x: array([-1.818813,  0.632525,  4.071022], dtype=float32)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            y: array([[-1.818813,  0.632525,  4.071022]], dtype=float32)\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f86af5d3f40>, array([-1.8188128 ,  0.63252497,  4.071022  ], dtype=float32), array([[-1.8188128 ,  0.63252497,  4.071022  ]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f86afa577f0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f86af5b0250>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m_________________________ test_summation[cpu-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.51554435 -1.0567564 ]\n",
      "  [-0.30265552  0.4281822 ]\n",
      "  [-0.68831074  1.6516227 ]]\n",
      "\n",
      " [[ 0.74296576 -0....4]\n",
      "  [-2.0539176  -0.6141424 ]]\n",
      "\n",
      " [[ 0.68060917  0.5635482 ]\n",
      "  [ 1.0085729   0.75971615]\n",
      "  [ 0.31350297 -2.0439312 ]]])\n",
      "_A         = array([[[-0.51554435, -1.0567564 ],\n",
      "        [-0.30265552,  0.4281822 ],\n",
      "        [-0.68831074,  1.6516227 ]],\n",
      "\n",
      "       [...  [[ 0.68060917,  0.5635482 ],\n",
      "        [ 1.0085729 ,  0.75971615],\n",
      "        [ 0.31350297, -2.0439312 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f86afa54af0>, array([[-1.5065106 ,  1.0230485 ],\n",
      "       [ 2.7984278 ...2.2587793 , -1.9422952 ]],\n",
      "\n",
      "       [[-0.6335145 , -1.4574671 ]],\n",
      "\n",
      "       [[ 2.002685  , -0.7206669 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           (shapes (8, 2), (8, 1, 2) mismatch)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            x: array([[-1.506511,  1.023049],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                  [ 2.798428, -0.53597 ],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                  [ 1.655922,  0.843713],...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            y: array([[[-1.506511,  1.023049]],\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE                  [[ 2.798428, -0.53597 ]],...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f86afa54af0>, array([[-1.5065106 ,  1.0230485 ],\n",
      "       [ 2.7984278 ...2.2587793 , -1.9422952 ]],\n",
      "\n",
      "       [[-0.6335145 , -1.4574671 ]],\n",
      "\n",
      "       [[ 2.002685  , -0.7206669 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f86afa577f0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f86af5b0250>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m_________________________ test_summation[cpu-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.09097444  0.11011965]\n",
      "  [-0.23238643 -1.2567368 ]\n",
      "  [-0.00447594  0.03316377]]\n",
      "\n",
      " [[ 0.4079615  -0....3]\n",
      "  [ 0.06753551  0.14588203]]\n",
      "\n",
      " [[ 0.4651775  -0.96335906]\n",
      "  [-0.22114491  1.0182978 ]\n",
      "  [ 0.30359092 -1.609142  ]]])\n",
      "_A         = array([[[-0.09097444,  0.11011965],\n",
      "        [-0.23238643, -1.2567368 ],\n",
      "        [-0.00447594,  0.03316377]],\n",
      "\n",
      "       [...  [[ 0.4651775 , -0.96335906],\n",
      "        [-0.22114491,  1.0182978 ],\n",
      "        [ 0.30359092, -1.609142  ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f86af5d3eb0>, array([[ 0.01914521, -1.4891232 ,  0.02868782],\n",
      "      ...7131 ],\n",
      "        [ 0.21341753]],\n",
      "\n",
      "       [[-0.49818155],\n",
      "        [ 0.7971529 ],\n",
      "        [-1.305551  ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           (shapes (8, 3), (8, 3, 1) mismatch)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            x: array([[ 0.019145, -1.489123,  0.028688],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                  [ 0.276608, -1.025892, -0.568414],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                  [-1.549419, -0.465551,  0.802755],...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            y: array([[[ 0.019145],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-1.489123],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 0.028688]],...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f86af5d3eb0>, array([[ 0.01914521, -1.4891232 ,  0.02868782],\n",
      "      ...7131 ],\n",
      "        [ 0.21341753]],\n",
      "\n",
      "       [[-0.49818155],\n",
      "        [ 0.7971529 ],\n",
      "        [-1.305551  ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f86afa577f0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f86af5b0250>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m________________________ test_summation[cuda-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[ 0.19680066 -0.31015334  0.9066847 ]\n",
      " [-0.15136234  0.8248054   0.588381  ]\n",
      " [ 1.102897    0.14859103  0.86367875]\n",
      " [ 0.49103832  0.46082434  0.2423399 ]\n",
      " [ 2.015139    0.10407976  0.60018444]])\n",
      "_A         = array([[ 0.19680066, -0.31015334,  0.9066847 ],\n",
      "       [-0.15136234,  0.8248054 ,  0.588381  ],\n",
      "       [ 1.102897  ,  ...67875],\n",
      "       [ 0.49103832,  0.46082434,  0.2423399 ],\n",
      "       [ 2.015139  ,  0.10407976,  0.60018444]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cuda()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f86af5d3c70>, array([3.654513 , 1.2281471, 3.2012687], dtype=float32), array([[3.654513 , 1.2281471, 3.2012687]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           (shapes (3,), (1, 3) mismatch)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            x: array([3.654513, 1.228147, 3.201269], dtype=float32)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            y: array([[3.654513, 1.228147, 3.201269]], dtype=float32)\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f86af5d3c70>, array([3.654513 , 1.2281471, 3.2012687], dtype=float32), array([[3.654513 , 1.2281471, 3.2012687]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f86afa577f0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f86af5b0250>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m________________________ test_summation[cuda-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.3742769  -0.15478124]\n",
      "  [-0.5490968   0.1884813 ]\n",
      "  [-0.6726331  -0.5823771 ]]\n",
      "\n",
      " [[ 1.1785907  -0.... ]\n",
      "  [-1.0777732   0.85772526]]\n",
      "\n",
      " [[ 0.72896844 -1.4150094 ]\n",
      "  [ 0.39689642 -2.1479504 ]\n",
      "  [ 0.84768903 -0.5966049 ]]])\n",
      "_A         = array([[[-0.3742769 , -0.15478124],\n",
      "        [-0.5490968 ,  0.1884813 ],\n",
      "        [-0.6726331 , -0.5823771 ]],\n",
      "\n",
      "       [...  [[ 0.72896844, -1.4150094 ],\n",
      "        [ 0.39689642, -2.1479504 ],\n",
      "        [ 0.84768903, -0.5966049 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f86a1c140d0>, array([[-1.5960069, -0.548677 ],\n",
      "       [ 2.5499053, -...   [[-1.0420849,  1.9881895]],\n",
      "\n",
      "       [[-1.7602795, -2.103706 ]],\n",
      "\n",
      "       [[ 1.9735539, -4.1595645]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           (shapes (8, 2), (8, 1, 2) mismatch)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            x: array([[-1.596007, -0.548677],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                  [ 2.549905, -2.160587],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                  [ 2.29408 ,  0.44325 ],...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            y: array([[[-1.596007, -0.548677]],\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE                  [[ 2.549905, -2.160587]],...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f86a1c140d0>, array([[-1.5960069, -0.548677 ],\n",
      "       [ 2.5499053, -...   [[-1.0420849,  1.9881895]],\n",
      "\n",
      "       [[-1.7602795, -2.103706 ]],\n",
      "\n",
      "       [[ 1.9735539, -4.1595645]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f86afa577f0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f86af5b0250>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m________________________ test_summation[cuda-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.43343157 -0.5803195 ]\n",
      "  [-0.79609615  0.6915893 ]\n",
      "  [ 0.4549692   0.6378628 ]]\n",
      "\n",
      " [[ 0.43467233  1....4]\n",
      "  [ 0.40885907  1.5238507 ]]\n",
      "\n",
      " [[ 1.8298346  -0.45180562]\n",
      "  [-0.16761442  0.9780772 ]\n",
      "  [ 0.9505592   0.0114131 ]]])\n",
      "_A         = array([[[-0.43343157, -0.5803195 ],\n",
      "        [-0.79609615,  0.6915893 ],\n",
      "        [ 0.4549692 ,  0.6378628 ]],\n",
      "\n",
      "       [...  [[ 1.8298346 , -0.45180562],\n",
      "        [-0.16761442,  0.9780772 ],\n",
      "        [ 0.9505592 ,  0.0114131 ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x7f86a1c14700>, array([[-1.013751  , -0.10450685,  1.092832  ],\n",
      "      ...57705],\n",
      "        [ 1.9327097 ]],\n",
      "\n",
      "       [[ 1.378029  ],\n",
      "        [ 0.8104628 ],\n",
      "        [ 0.9619723 ]]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           (shapes (8, 3), (8, 3, 1) mismatch)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            x: array([[-1.013751, -0.104507,  1.092832],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                  [ 1.763682, -2.601305,  2.197898],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                  [ 0.338505,  1.067568, -1.375669],...\u001b[0m\n",
      "\u001b[1m\u001b[31mE            y: array([[[-1.013751],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [-0.104507],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                   [ 1.092832]],...\u001b[0m\n",
      "\n",
      "args       = (<function assert_allclose.<locals>.compare at 0x7f86a1c14700>, array([[-1.013751  , -0.10450685,  1.092832  ],\n",
      "      ...57705],\n",
      "        [ 1.9327097 ]],\n",
      "\n",
      "       [[ 1.378029  ],\n",
      "        [ 0.8104628 ],\n",
      "        [ 0.9619723 ]]], dtype=float32))\n",
      "func       = <function assert_array_compare at 0x7f86afa577f0>\n",
      "kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "self       = <contextlib._GeneratorContextManager object at 0x7f86af5b0250>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/contextlib.py\u001b[0m:79: AssertionError\n",
      "\u001b[31m\u001b[1m___________________ test_summation_backward[cpu-shape0-None] ___________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[0.25179565]]])\n",
      "_A         = array([[[0.25179562]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    \u001b[0mbackward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.25179565]]]),)\n",
      "        c          = array([[[-0.26194048]]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f876160dab0>\n",
      "        f1         = -0.06595808950159887\n",
      "        f2         = -0.06595285138387812\n",
      "        i          = 0\n",
      "        j          = 0\n",
      "        kwargs     = {'axes': None}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[-0.26190589]]])]\n",
      "        out        = needle.Tensor([[[0.25179562]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "        node       = needle.Tensor([[[0.25179562]]])\n",
      "        out_grad   = needle.Tensor([[[-0.26194048]]])\n",
      "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bc2dd0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bc2dd0>\n",
      "out_grad = needle.Tensor([[[-0.26194048]]])\n",
      "node = needle.Tensor([[[0.25179562]]])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m, out_grad: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, node: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, Tuple[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Compute partial adjoint for each input value for a given output adjoint.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Parameters\u001b[39;49;00m\n",
      "    \u001b[33m    ----------\u001b[39;49;00m\n",
      "    \u001b[33m    out_grad: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The adjoint wrt to the output value.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    node: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The value node of forward evaluation.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns\u001b[39;49;00m\n",
      "    \u001b[33m    -------\u001b[39;49;00m\n",
      "    \u001b[33m    input_grads: Value or Tuple[Value]\u001b[39;49;00m\n",
      "    \u001b[33m        A list containing partial gradient adjoints to be propagated to\u001b[39;49;00m\n",
      "    \u001b[33m        each of the input node.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "node       = needle.Tensor([[[0.25179562]]])\n",
      "out_grad   = needle.Tensor([[[-0.26194048]]])\n",
      "self       = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bc2dd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:63: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape1-0] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[ 0.36832857  0.524743    0.72734344]\n",
      " [-0.6690794   0.2118702  -0.17624503]\n",
      " [-0.5157768  -1.9483194   0.21689712]\n",
      " [ 0.43637756 -0.49504334  1.4166641 ]\n",
      " [-0.26324236  1.5031598  -1.2881242 ]])\n",
      "_A         = array([[ 0.36832854,  0.524743  ,  0.72734344],\n",
      "       [-0.6690794 ,  0.2118702 , -0.17624503],\n",
      "       [-0.5157768 , -...89712],\n",
      "       [ 0.43637753, -0.49504337,  1.4166641 ],\n",
      "       [-0.2632424 ,  1.5031598 , -1.2881242 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cpu()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    \u001b[0mbackward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 0.36832857  0.524743    0.72734344]\n",
      " [-0.6690794   0.2118702  -0.17624503]\n",
      " [-0.5157768  -1.9483194   0.21689712]\n",
      " [ 0.43637756 -0.49504334  1.4166641 ]\n",
      " [-0.26324236  1.5031598  -1.2881242 ]]),)\n",
      "        c          = array([[-0.24279231,  0.51338752,  1.17676622]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f876160dab0>\n",
      "        f1         = 1.1067148318683209\n",
      "        f2         = 1.1066912645822016\n",
      "        i          = 0\n",
      "        j          = 14\n",
      "        kwargs     = {'axes': 0}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[-0.24312203,  0.51408472,  1.17836431],\n",
      "       [-0.24312203,  0.51408472,  1.17836431],\n",
      "       [-0.24312203, ...408472,  1.16433616],\n",
      "       [-0.24239845,  0.51102469,  1.17836431],\n",
      "       [-0.24239845,  0.51408472,  1.17836431]])]\n",
      "        out        = needle.Tensor([[-0.64339256 -0.2035898   0.8965355 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "        node       = needle.Tensor([[-0.64339256 -0.2035898   0.8965355 ]])\n",
      "        out_grad   = needle.Tensor([[-0.24279231  0.5133875   1.1767663 ]])\n",
      "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bb9810>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bb9810>\n",
      "out_grad = needle.Tensor([[-0.24279231  0.5133875   1.1767663 ]])\n",
      "node = needle.Tensor([[-0.64339256 -0.2035898   0.8965355 ]])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m, out_grad: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, node: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, Tuple[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Compute partial adjoint for each input value for a given output adjoint.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Parameters\u001b[39;49;00m\n",
      "    \u001b[33m    ----------\u001b[39;49;00m\n",
      "    \u001b[33m    out_grad: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The adjoint wrt to the output value.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    node: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The value node of forward evaluation.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns\u001b[39;49;00m\n",
      "    \u001b[33m    -------\u001b[39;49;00m\n",
      "    \u001b[33m    input_grads: Value or Tuple[Value]\u001b[39;49;00m\n",
      "    \u001b[33m        A list containing partial gradient adjoints to be propagated to\u001b[39;49;00m\n",
      "    \u001b[33m        each of the input node.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "node       = needle.Tensor([[-0.64339256 -0.2035898   0.8965355 ]])\n",
      "out_grad   = needle.Tensor([[-0.24279231  0.5133875   1.1767663 ]])\n",
      "self       = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bb9810>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:63: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape2-1] _____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-1.223357    1.9838535 ]\n",
      "  [-0.40351704  1.5878327 ]\n",
      "  [ 1.8947744   1.2847524 ]]\n",
      "\n",
      " [[-0.955546   -0....3]\n",
      "  [ 0.66463155  0.21836282]]\n",
      "\n",
      " [[ 0.6723505  -0.5805451 ]\n",
      "  [ 0.94992834 -0.13286018]\n",
      "  [-0.05775289  2.3362038 ]]])\n",
      "_A         = array([[[-1.223357  ,  1.9838535 ],\n",
      "        [-0.40351707,  1.5878327 ],\n",
      "        [ 1.8947744 ,  1.2847524 ]],\n",
      "\n",
      "       [...  [[ 0.6723505 , -0.5805451 ],\n",
      "        [ 0.94992834, -0.13286018],\n",
      "        [-0.05775288,  2.3362038 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    \u001b[0mbackward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.223357    1.9838535 ]\n",
      "  [-0.40351704  1.5878327 ]\n",
      "  [ 1.8947744   1.2847524 ]]\n",
      "\n",
      " [[-0.955546   -0...\n",
      "  [ 0.66463155  0.21836282]]\n",
      "\n",
      " [[ 0.6723505  -0.5805451 ]\n",
      "  [ 0.94992834 -0.13286018]\n",
      "  [-0.05775289  2.3362038 ]]]),)\n",
      "        c          = array([[[-0.02063232,  0.17829331]],\n",
      "\n",
      "       [[ 1.29770819,  1.22375498]],\n",
      "\n",
      "       [[ 0.56194033,  0.57480341]],\n",
      "\n",
      "    ... ]],\n",
      "\n",
      "       [[-0.85796767,  0.14308997]],\n",
      "\n",
      "       [[-1.59867454, -3.0235811 ]],\n",
      "\n",
      "       [[ 0.22697888, -1.57959625]]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f876160dab0>\n",
      "        f1         = -4.059700185351925\n",
      "        f2         = -4.0596685505241386\n",
      "        i          = 0\n",
      "        j          = 47\n",
      "        kwargs     = {'axes': 1}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[-0.02066034,  0.17853544],\n",
      "        [-0.02066034,  0.17853544],\n",
      "        [-0.02066034,  0.17853544]],\n",
      "\n",
      "       ...7625]],\n",
      "\n",
      "       [[ 0.22728712, -1.58174139],\n",
      "        [ 0.22728712, -1.58174139],\n",
      "        [ 0.22728712, -1.58174139]]])]\n",
      "        out        = needle.Tensor([[[ 0.26790047  4.8564386 ]]\n",
      "\n",
      " [[-0.68675244 -1.1846567 ]]\n",
      "\n",
      " [[ 0.08269829  1.9032902 ]]\n",
      "\n",
      " [[ 0.05148055...[[ 1.3483511   2.799955  ]]\n",
      "\n",
      " [[ 0.762841    0.44975713]]\n",
      "\n",
      " [[-0.68042046 -0.5701694 ]]\n",
      "\n",
      " [[ 1.5645261   1.6227984 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "        node       = needle.Tensor([[[ 0.26790047  4.8564386 ]]\n",
      "\n",
      " [[-0.68675244 -1.1846567 ]]\n",
      "\n",
      " [[ 0.08269829  1.9032902 ]]\n",
      "\n",
      " [[ 0.05148055...[[ 1.3483511   2.799955  ]]\n",
      "\n",
      " [[ 0.762841    0.44975713]]\n",
      "\n",
      " [[-0.68042046 -0.5701694 ]]\n",
      "\n",
      " [[ 1.5645261   1.6227984 ]]])\n",
      "        out_grad   = needle.Tensor([[[-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.223755  ]]\n",
      "\n",
      " [[ 0.5619403   0.5748034 ]]\n",
      "\n",
      " [[-0.39962965...[[-1.5687171  -0.5583447 ]]\n",
      "\n",
      " [[-0.8579677   0.14308996]]\n",
      "\n",
      " [[-1.5986745  -3.023581  ]]\n",
      "\n",
      " [[ 0.22697888 -1.5795963 ]]])\n",
      "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bc1690>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bc1690>\n",
      "out_grad = needle.Tensor([[[-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.223755  ]]\n",
      "\n",
      " [[ 0.5619403   0.5748034 ]]\n",
      "\n",
      " [[-0.39962965...[[-1.5687171  -0.5583447 ]]\n",
      "\n",
      " [[-0.8579677   0.14308996]]\n",
      "\n",
      " [[-1.5986745  -3.023581  ]]\n",
      "\n",
      " [[ 0.22697888 -1.5795963 ]]])\n",
      "node = needle.Tensor([[[ 0.26790047  4.8564386 ]]\n",
      "\n",
      " [[-0.68675244 -1.1846567 ]]\n",
      "\n",
      " [[ 0.08269829  1.9032902 ]]\n",
      "\n",
      " [[ 0.05148055...[[ 1.3483511   2.799955  ]]\n",
      "\n",
      " [[ 0.762841    0.44975713]]\n",
      "\n",
      " [[-0.68042046 -0.5701694 ]]\n",
      "\n",
      " [[ 1.5645261   1.6227984 ]]])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m, out_grad: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, node: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, Tuple[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Compute partial adjoint for each input value for a given output adjoint.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Parameters\u001b[39;49;00m\n",
      "    \u001b[33m    ----------\u001b[39;49;00m\n",
      "    \u001b[33m    out_grad: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The adjoint wrt to the output value.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    node: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The value node of forward evaluation.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns\u001b[39;49;00m\n",
      "    \u001b[33m    -------\u001b[39;49;00m\n",
      "    \u001b[33m    input_grads: Value or Tuple[Value]\u001b[39;49;00m\n",
      "    \u001b[33m        A list containing partial gradient adjoints to be propagated to\u001b[39;49;00m\n",
      "    \u001b[33m        each of the input node.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "node       = needle.Tensor([[[ 0.26790047  4.8564386 ]]\n",
      "\n",
      " [[-0.68675244 -1.1846567 ]]\n",
      "\n",
      " [[ 0.08269829  1.9032902 ]]\n",
      "\n",
      " [[ 0.05148055...[[ 1.3483511   2.799955  ]]\n",
      "\n",
      " [[ 0.762841    0.44975713]]\n",
      "\n",
      " [[-0.68042046 -0.5701694 ]]\n",
      "\n",
      " [[ 1.5645261   1.6227984 ]]])\n",
      "out_grad   = needle.Tensor([[[-0.02063232  0.17829332]]\n",
      "\n",
      " [[ 1.2977082   1.223755  ]]\n",
      "\n",
      " [[ 0.5619403   0.5748034 ]]\n",
      "\n",
      " [[-0.39962965...[[-1.5687171  -0.5583447 ]]\n",
      "\n",
      " [[-0.8579677   0.14308996]]\n",
      "\n",
      " [[-1.5986745  -3.023581  ]]\n",
      "\n",
      " [[ 0.22697888 -1.5795963 ]]])\n",
      "self       = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bc1690>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:63: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape3-2] _____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 0.00771675 -0.477715  ]\n",
      "  [-0.37093356 -1.6240172 ]\n",
      "  [-1.6641127   0.5510251 ]]\n",
      "\n",
      " [[-0.24003424 -0....6]\n",
      "  [ 0.06870755 -1.2057985 ]]\n",
      "\n",
      " [[-1.0558244   1.0220021 ]\n",
      "  [-0.20333268  0.9668824 ]\n",
      "  [-0.25639063  0.5157999 ]]])\n",
      "_A         = array([[[ 0.00771675, -0.47771502],\n",
      "        [-0.3709336 , -1.6240172 ],\n",
      "        [-1.6641127 ,  0.5510251 ]],\n",
      "\n",
      "       [...  [[-1.0558244 ,  1.0220021 ],\n",
      "        [-0.20333268,  0.9668824 ],\n",
      "        [-0.25639066,  0.5157999 ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    \u001b[0mbackward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 0.00771675 -0.477715  ]\n",
      "  [-0.37093356 -1.6240172 ]\n",
      "  [-1.6641127   0.5510251 ]]\n",
      "\n",
      " [[-0.24003424 -0...\n",
      "  [ 0.06870755 -1.2057985 ]]\n",
      "\n",
      " [[-1.0558244   1.0220021 ]\n",
      "  [-0.20333268  0.9668824 ]\n",
      "  [-0.25639063  0.5157999 ]]]),)\n",
      "        c          = array([[[-0.44359782],\n",
      "        [ 0.83550506],\n",
      "        [-0.75412982]],\n",
      "\n",
      "       [[-1.34921992],\n",
      "        [ 0.28219448],\n",
      " ...\n",
      "        [-0.38329579],\n",
      "        [ 0.27533789]],\n",
      "\n",
      "       [[-0.95338377],\n",
      "        [ 0.44119945],\n",
      "        [-0.18395162]]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f876160dab0>\n",
      "        f1         = 0.42565076588967754\n",
      "        f2         = 0.42565444991823\n",
      "        i          = 0\n",
      "        j          = 47\n",
      "        kwargs     = {'axes': 2}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[-0.44353923, -0.44353923],\n",
      "        [ 0.8366397 ,  0.8366397 ],\n",
      "        [-0.75515395, -0.75515395]],\n",
      "\n",
      "       ...1181]],\n",
      "\n",
      "       [[-0.9546785 , -0.9546785 ],\n",
      "        [ 0.44048374,  0.44179862],\n",
      "        [-0.18392732, -0.18420143]]])]\n",
      "        out        = needle.Tensor([[[-0.46999827]\n",
      "  [-1.9949508 ]\n",
      "  [-1.1130877 ]]\n",
      "\n",
      " [[-1.2167768 ]\n",
      "  [-0.14212924]\n",
      "  [-2.7648933 ]]\n",
      "\n",
      " [[-...\n",
      "  [-0.72803926]]\n",
      "\n",
      " [[ 1.6400697 ]\n",
      "  [ 2.2829142 ]\n",
      "  [-1.1370909 ]]\n",
      "\n",
      " [[-0.0338223 ]\n",
      "  [ 0.76354975]\n",
      "  [ 0.25940922]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "        node       = needle.Tensor([[[-0.46999827]\n",
      "  [-1.9949508 ]\n",
      "  [-1.1130877 ]]\n",
      "\n",
      " [[-1.2167768 ]\n",
      "  [-0.14212924]\n",
      "  [-2.7648933 ]]\n",
      "\n",
      " [[-...\n",
      "  [-0.72803926]]\n",
      "\n",
      " [[ 1.6400697 ]\n",
      "  [ 2.2829142 ]\n",
      "  [-1.1370909 ]]\n",
      "\n",
      " [[-0.0338223 ]\n",
      "  [ 0.76354975]\n",
      "  [ 0.25940922]]])\n",
      "        out_grad   = needle.Tensor([[[-0.44359782]\n",
      "  [ 0.83550507]\n",
      "  [-0.7541298 ]]\n",
      "\n",
      " [[-1.3492199 ]\n",
      "  [ 0.28219447]\n",
      "  [ 0.2446352 ]]\n",
      "\n",
      " [[ ...\n",
      "  [ 1.0652852 ]]\n",
      "\n",
      " [[ 1.8692191 ]\n",
      "  [-0.38329577]\n",
      "  [ 0.27533787]]\n",
      "\n",
      " [[-0.95338374]\n",
      "  [ 0.44119945]\n",
      "  [-0.18395162]]])\n",
      "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bc7d30>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bc7d30>\n",
      "out_grad = needle.Tensor([[[-0.44359782]\n",
      "  [ 0.83550507]\n",
      "  [-0.7541298 ]]\n",
      "\n",
      " [[-1.3492199 ]\n",
      "  [ 0.28219447]\n",
      "  [ 0.2446352 ]]\n",
      "\n",
      " [[ ...\n",
      "  [ 1.0652852 ]]\n",
      "\n",
      " [[ 1.8692191 ]\n",
      "  [-0.38329577]\n",
      "  [ 0.27533787]]\n",
      "\n",
      " [[-0.95338374]\n",
      "  [ 0.44119945]\n",
      "  [-0.18395162]]])\n",
      "node = needle.Tensor([[[-0.46999827]\n",
      "  [-1.9949508 ]\n",
      "  [-1.1130877 ]]\n",
      "\n",
      " [[-1.2167768 ]\n",
      "  [-0.14212924]\n",
      "  [-2.7648933 ]]\n",
      "\n",
      " [[-...\n",
      "  [-0.72803926]]\n",
      "\n",
      " [[ 1.6400697 ]\n",
      "  [ 2.2829142 ]\n",
      "  [-1.1370909 ]]\n",
      "\n",
      " [[-0.0338223 ]\n",
      "  [ 0.76354975]\n",
      "  [ 0.25940922]]])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m, out_grad: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, node: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, Tuple[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Compute partial adjoint for each input value for a given output adjoint.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Parameters\u001b[39;49;00m\n",
      "    \u001b[33m    ----------\u001b[39;49;00m\n",
      "    \u001b[33m    out_grad: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The adjoint wrt to the output value.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    node: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The value node of forward evaluation.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns\u001b[39;49;00m\n",
      "    \u001b[33m    -------\u001b[39;49;00m\n",
      "    \u001b[33m    input_grads: Value or Tuple[Value]\u001b[39;49;00m\n",
      "    \u001b[33m        A list containing partial gradient adjoints to be propagated to\u001b[39;49;00m\n",
      "    \u001b[33m        each of the input node.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "node       = needle.Tensor([[[-0.46999827]\n",
      "  [-1.9949508 ]\n",
      "  [-1.1130877 ]]\n",
      "\n",
      " [[-1.2167768 ]\n",
      "  [-0.14212924]\n",
      "  [-2.7648933 ]]\n",
      "\n",
      " [[-...\n",
      "  [-0.72803926]]\n",
      "\n",
      " [[ 1.6400697 ]\n",
      "  [ 2.2829142 ]\n",
      "  [-1.1370909 ]]\n",
      "\n",
      " [[-0.0338223 ]\n",
      "  [ 0.76354975]\n",
      "  [ 0.25940922]]])\n",
      "out_grad   = needle.Tensor([[[-0.44359782]\n",
      "  [ 0.83550507]\n",
      "  [-0.7541298 ]]\n",
      "\n",
      " [[-1.3492199 ]\n",
      "  [ 0.28219447]\n",
      "  [ 0.2446352 ]]\n",
      "\n",
      " [[ ...\n",
      "  [ 1.0652852 ]]\n",
      "\n",
      " [[ 1.8692191 ]\n",
      "  [-0.38329577]\n",
      "  [ 0.27533787]]\n",
      "\n",
      " [[-0.95338374]\n",
      "  [ 0.44119945]\n",
      "  [-0.18395162]]])\n",
      "self       = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bc7d30>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:63: NotImplementedError\n",
      "\u001b[31m\u001b[1m__________________ test_summation_backward[cuda-shape0-None] ___________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[1.0349756]]])\n",
      "_A         = array([[[1.0349756]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    \u001b[0mbackward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[1.0349756]]]),)\n",
      "        c          = array([[[1.15621613]]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f876160dab0>\n",
      "        f1         = 1.1966671186806974\n",
      "        f2         = 1.1966439629544732\n",
      "        i          = 0\n",
      "        j          = 0\n",
      "        kwargs     = {'axes': None}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[1.15778631]]])]\n",
      "        out        = needle.Tensor([[[1.0349756]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "        node       = needle.Tensor([[[1.0349756]]])\n",
      "        out_grad   = needle.Tensor([[[1.1562161]]])\n",
      "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bbd480>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bbd480>\n",
      "out_grad = needle.Tensor([[[1.1562161]]]), node = needle.Tensor([[[1.0349756]]])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m, out_grad: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, node: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, Tuple[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Compute partial adjoint for each input value for a given output adjoint.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Parameters\u001b[39;49;00m\n",
      "    \u001b[33m    ----------\u001b[39;49;00m\n",
      "    \u001b[33m    out_grad: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The adjoint wrt to the output value.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    node: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The value node of forward evaluation.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns\u001b[39;49;00m\n",
      "    \u001b[33m    -------\u001b[39;49;00m\n",
      "    \u001b[33m    input_grads: Value or Tuple[Value]\u001b[39;49;00m\n",
      "    \u001b[33m        A list containing partial gradient adjoints to be propagated to\u001b[39;49;00m\n",
      "    \u001b[33m        each of the input node.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "node       = needle.Tensor([[[1.0349756]]])\n",
      "out_grad   = needle.Tensor([[[1.1562161]]])\n",
      "self       = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bbd480>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:63: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape1-0] ____________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[-0.40711153 -0.80170184 -1.1139419 ]\n",
      " [ 0.3914071  -1.674953   -0.23902948]\n",
      " [ 0.7553679  -1.4470354   1.9484388 ]\n",
      " [ 0.5264981  -0.20674394  1.478983  ]\n",
      " [ 0.37850133 -1.1597295   0.8766111 ]])\n",
      "_A         = array([[-0.40711156, -0.80170184, -1.1139419 ],\n",
      "       [ 0.39140707, -1.674953  , -0.23902948],\n",
      "       [ 0.7553679 , -...4388 ],\n",
      "       [ 0.5264981 , -0.20674394,  1.478983  ],\n",
      "       [ 0.3785013 , -1.1597295 ,  0.8766111 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cuda()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    \u001b[0mbackward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[-0.40711153 -0.80170184 -1.1139419 ]\n",
      " [ 0.3914071  -1.674953   -0.23902948]\n",
      " [ 0.7553679  -1.4470354   1.9484388 ]\n",
      " [ 0.5264981  -0.20674394  1.478983  ]\n",
      " [ 0.37850133 -1.1597295   0.8766111 ]]),)\n",
      "        c          = array([[-1.75722884,  0.31880177, -1.65264358]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f876160dab0>\n",
      "        f1         = -9.45363223369758\n",
      "        f2         = -9.453599135939179\n",
      "        i          = 0\n",
      "        j          = 14\n",
      "        kwargs     = {'axes': 0}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[-1.75961521,  0.31923471, -1.65488792],\n",
      "       [-1.74914131,  0.31923471, -1.65488792],\n",
      "       [-1.75961521, ...923471, -1.65488792],\n",
      "       [-1.75961521,  0.31163388, -1.65488792],\n",
      "       [-1.75961521,  0.31923471, -1.65488792]])]\n",
      "        out        = needle.Tensor([[ 1.6446627 -5.2901635  2.9510617]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "        node       = needle.Tensor([[ 1.6446627 -5.2901635  2.9510617]])\n",
      "        out_grad   = needle.Tensor([[-1.7572289   0.31880176 -1.6526436 ]])\n",
      "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f86af5b1990>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Summation object at 0x7f86af5b1990>\n",
      "out_grad = needle.Tensor([[-1.7572289   0.31880176 -1.6526436 ]])\n",
      "node = needle.Tensor([[ 1.6446627 -5.2901635  2.9510617]])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m, out_grad: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, node: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, Tuple[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Compute partial adjoint for each input value for a given output adjoint.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Parameters\u001b[39;49;00m\n",
      "    \u001b[33m    ----------\u001b[39;49;00m\n",
      "    \u001b[33m    out_grad: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The adjoint wrt to the output value.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    node: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The value node of forward evaluation.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns\u001b[39;49;00m\n",
      "    \u001b[33m    -------\u001b[39;49;00m\n",
      "    \u001b[33m    input_grads: Value or Tuple[Value]\u001b[39;49;00m\n",
      "    \u001b[33m        A list containing partial gradient adjoints to be propagated to\u001b[39;49;00m\n",
      "    \u001b[33m        each of the input node.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "node       = needle.Tensor([[ 1.6446627 -5.2901635  2.9510617]])\n",
      "out_grad   = needle.Tensor([[-1.7572289   0.31880176 -1.6526436 ]])\n",
      "self       = <needle.ops.ops_mathematic.Summation object at 0x7f86af5b1990>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:63: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape2-1] ____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 3.1887996e-01  1.5159521e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "\n",
      "...2054883e-01]]\n",
      "\n",
      " [[ 1.4263909e+00  3.2395303e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]])\n",
      "_A         = array([[[ 3.1887993e-01,  1.5159522e-03],\n",
      "        [-9.1810602e-01,  8.5386848e-01],\n",
      "        [ 5.3351974e-01,  6.397005...,  3.2395300e-01],\n",
      "        [ 2.3655061e-01,  2.1437683e+00],\n",
      "        [ 1.1814048e+00, -1.3010720e+00]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    \u001b[0mbackward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 3.1887996e-01  1.5159521e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "...54883e-01]]\n",
      "\n",
      " [[ 1.4263909e+00  3.2395303e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]]),)\n",
      "        c          = array([[[-0.43455654,  0.55048798]],\n",
      "\n",
      "       [[ 0.06143115,  1.20697315]],\n",
      "\n",
      "       [[ 0.25282092, -0.36453617]],\n",
      "\n",
      "    ...6]],\n",
      "\n",
      "       [[ 0.05111475,  1.69073882]],\n",
      "\n",
      "       [[ 1.66974421, -1.40444807]],\n",
      "\n",
      "       [[-1.4388192 , -0.02291146]]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f876160dab0>\n",
      "        f1         = 16.73120485004283\n",
      "        f2         = 16.73120530889439\n",
      "        i          = 0\n",
      "        j          = 47\n",
      "        kwargs     = {'axes': 1}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[-0.43385161,  0.55123556],\n",
      "        [-0.43514669,  0.55123556],\n",
      "        [-0.43514669,  0.55123556]],\n",
      "\n",
      "       ...6256]],\n",
      "\n",
      "       [[-1.44077317, -0.02266945],\n",
      "        [-1.44077317, -0.02294258],\n",
      "        [-1.44077317, -0.02294258]]])]\n",
      "        out        = needle.Tensor([[[-0.06570637  1.495085  ]]\n",
      "\n",
      " [[-0.77701896 -0.59986424]]\n",
      "\n",
      " [[ 0.78010964 -2.325438  ]]\n",
      "\n",
      " [[-0.16809821...[[ 1.8090553  -2.381361  ]]\n",
      "\n",
      " [[ 0.66218865  2.0200748 ]]\n",
      "\n",
      " [[ 1.4608432   0.38579413]]\n",
      "\n",
      " [[ 2.8443463   1.1666492 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "        node       = needle.Tensor([[[-0.06570637  1.495085  ]]\n",
      "\n",
      " [[-0.77701896 -0.59986424]]\n",
      "\n",
      " [[ 0.78010964 -2.325438  ]]\n",
      "\n",
      " [[-0.16809821...[[ 1.8090553  -2.381361  ]]\n",
      "\n",
      " [[ 0.66218865  2.0200748 ]]\n",
      "\n",
      " [[ 1.4608432   0.38579413]]\n",
      "\n",
      " [[ 2.8443463   1.1666492 ]]])\n",
      "        out_grad   = needle.Tensor([[[-0.43455654  0.550488  ]]\n",
      "\n",
      " [[ 0.06143114  1.2069732 ]]\n",
      "\n",
      " [[ 0.2528209  -0.36453617]]\n",
      "\n",
      " [[-0.1625579 ...[[ 1.5918057  -1.4382539 ]]\n",
      "\n",
      " [[ 0.05111475  1.6907388 ]]\n",
      "\n",
      " [[ 1.6697443  -1.404448  ]]\n",
      "\n",
      " [[-1.4388192  -0.02291146]]])\n",
      "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bf0910>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bf0910>\n",
      "out_grad = needle.Tensor([[[-0.43455654  0.550488  ]]\n",
      "\n",
      " [[ 0.06143114  1.2069732 ]]\n",
      "\n",
      " [[ 0.2528209  -0.36453617]]\n",
      "\n",
      " [[-0.1625579 ...[[ 1.5918057  -1.4382539 ]]\n",
      "\n",
      " [[ 0.05111475  1.6907388 ]]\n",
      "\n",
      " [[ 1.6697443  -1.404448  ]]\n",
      "\n",
      " [[-1.4388192  -0.02291146]]])\n",
      "node = needle.Tensor([[[-0.06570637  1.495085  ]]\n",
      "\n",
      " [[-0.77701896 -0.59986424]]\n",
      "\n",
      " [[ 0.78010964 -2.325438  ]]\n",
      "\n",
      " [[-0.16809821...[[ 1.8090553  -2.381361  ]]\n",
      "\n",
      " [[ 0.66218865  2.0200748 ]]\n",
      "\n",
      " [[ 1.4608432   0.38579413]]\n",
      "\n",
      " [[ 2.8443463   1.1666492 ]]])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m, out_grad: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, node: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, Tuple[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Compute partial adjoint for each input value for a given output adjoint.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Parameters\u001b[39;49;00m\n",
      "    \u001b[33m    ----------\u001b[39;49;00m\n",
      "    \u001b[33m    out_grad: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The adjoint wrt to the output value.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    node: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The value node of forward evaluation.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns\u001b[39;49;00m\n",
      "    \u001b[33m    -------\u001b[39;49;00m\n",
      "    \u001b[33m    input_grads: Value or Tuple[Value]\u001b[39;49;00m\n",
      "    \u001b[33m        A list containing partial gradient adjoints to be propagated to\u001b[39;49;00m\n",
      "    \u001b[33m        each of the input node.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "node       = needle.Tensor([[[-0.06570637  1.495085  ]]\n",
      "\n",
      " [[-0.77701896 -0.59986424]]\n",
      "\n",
      " [[ 0.78010964 -2.325438  ]]\n",
      "\n",
      " [[-0.16809821...[[ 1.8090553  -2.381361  ]]\n",
      "\n",
      " [[ 0.66218865  2.0200748 ]]\n",
      "\n",
      " [[ 1.4608432   0.38579413]]\n",
      "\n",
      " [[ 2.8443463   1.1666492 ]]])\n",
      "out_grad   = needle.Tensor([[[-0.43455654  0.550488  ]]\n",
      "\n",
      " [[ 0.06143114  1.2069732 ]]\n",
      "\n",
      " [[ 0.2528209  -0.36453617]]\n",
      "\n",
      " [[-0.1625579 ...[[ 1.5918057  -1.4382539 ]]\n",
      "\n",
      " [[ 0.05111475  1.6907388 ]]\n",
      "\n",
      " [[ 1.6697443  -1.404448  ]]\n",
      "\n",
      " [[-1.4388192  -0.02291146]]])\n",
      "self       = <needle.ops.ops_mathematic.Summation object at 0x7f86a1bf0910>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:63: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape3-2] ____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 0.04162927 -1.2710459 ]\n",
      "  [ 0.00565341  0.68353325]\n",
      "  [-1.1112776  -2.5426693 ]]\n",
      "\n",
      " [[ 0.51752055  0.... ]\n",
      "  [-0.25414678  1.4692454 ]]\n",
      "\n",
      " [[-0.25943646  1.2197713 ]\n",
      "  [-0.9723478  -0.6831798 ]\n",
      "  [ 0.37447047  0.16003066]]])\n",
      "_A         = array([[[ 0.04162927, -1.2710459 ],\n",
      "        [ 0.00565341,  0.68353325],\n",
      "        [-1.1112776 , -2.5426693 ]],\n",
      "\n",
      "       [...  [[-0.2594365 ,  1.2197713 ],\n",
      "        [-0.9723478 , -0.6831798 ],\n",
      "        [ 0.37447044,  0.16003066]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    \u001b[0mbackward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 0.04162927 -1.2710459 ]\n",
      "  [ 0.00565341  0.68353325]\n",
      "  [-1.1112776  -2.5426693 ]]\n",
      "\n",
      " [[ 0.51752055  0...\n",
      "  [-0.25414678  1.4692454 ]]\n",
      "\n",
      " [[-0.25943646  1.2197713 ]\n",
      "  [-0.9723478  -0.6831798 ]\n",
      "  [ 0.37447047  0.16003066]]]),)\n",
      "        c          = array([[[ 1.27281975],\n",
      "        [ 1.10007122],\n",
      "        [-0.74093072]],\n",
      "\n",
      "       [[-0.50888016],\n",
      "        [ 0.40810799],\n",
      " ...\n",
      "        [ 0.18546481],\n",
      "        [-0.2667139 ]],\n",
      "\n",
      "       [[-0.31800157],\n",
      "        [-1.28237754],\n",
      "        [ 1.4503455 ]]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f876160dab0>\n",
      "        f1         = -3.5786765085401635\n",
      "        f2         = -3.578705554842454\n",
      "        i          = 0\n",
      "        j          = 47\n",
      "        kwargs     = {'axes': 2}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[ 1.27454828,  1.27454828],\n",
      "        [ 1.09828668,  1.10156515],\n",
      "        [-0.74193693, -0.74193693]],\n",
      "\n",
      "       ...761 ]],\n",
      "\n",
      "       [[-0.31843342, -0.31843342],\n",
      "        [-1.28411905, -1.28411905],\n",
      "        [ 1.44799275,  1.45231511]]])]\n",
      "        out        = needle.Tensor([[[-1.2294166 ]\n",
      "  [ 0.68918663]\n",
      "  [-3.6539469 ]]\n",
      "\n",
      " [[ 0.98761314]\n",
      "  [-0.05946481]\n",
      "  [-0.21467207]]\n",
      "\n",
      " [[ ...\n",
      "  [-0.49049693]]\n",
      "\n",
      " [[ 0.6714237 ]\n",
      "  [ 1.1756382 ]\n",
      "  [ 1.2150986 ]]\n",
      "\n",
      " [[ 0.9603348 ]\n",
      "  [-1.6555276 ]\n",
      "  [ 0.5345011 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "        node       = needle.Tensor([[[-1.2294166 ]\n",
      "  [ 0.68918663]\n",
      "  [-3.6539469 ]]\n",
      "\n",
      " [[ 0.98761314]\n",
      "  [-0.05946481]\n",
      "  [-0.21467207]]\n",
      "\n",
      " [[ ...\n",
      "  [-0.49049693]]\n",
      "\n",
      " [[ 0.6714237 ]\n",
      "  [ 1.1756382 ]\n",
      "  [ 1.2150986 ]]\n",
      "\n",
      " [[ 0.9603348 ]\n",
      "  [-1.6555276 ]\n",
      "  [ 0.5345011 ]]])\n",
      "        out_grad   = needle.Tensor([[[ 1.2728198 ]\n",
      "  [ 1.1000712 ]\n",
      "  [-0.74093074]]\n",
      "\n",
      " [[-0.50888014]\n",
      "  [ 0.408108  ]\n",
      "  [ 0.53634965]]\n",
      "\n",
      " [[-...\n",
      "  [-0.22966217]]\n",
      "\n",
      " [[-1.2457664 ]\n",
      "  [ 0.1854648 ]\n",
      "  [-0.2667139 ]]\n",
      "\n",
      " [[-0.31800157]\n",
      "  [-1.2823775 ]\n",
      "  [ 1.4503455 ]]])\n",
      "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f86afbebc10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Summation object at 0x7f86afbebc10>\n",
      "out_grad = needle.Tensor([[[ 1.2728198 ]\n",
      "  [ 1.1000712 ]\n",
      "  [-0.74093074]]\n",
      "\n",
      " [[-0.50888014]\n",
      "  [ 0.408108  ]\n",
      "  [ 0.53634965]]\n",
      "\n",
      " [[-...\n",
      "  [-0.22966217]]\n",
      "\n",
      " [[-1.2457664 ]\n",
      "  [ 0.1854648 ]\n",
      "  [-0.2667139 ]]\n",
      "\n",
      " [[-0.31800157]\n",
      "  [-1.2823775 ]\n",
      "  [ 1.4503455 ]]])\n",
      "node = needle.Tensor([[[-1.2294166 ]\n",
      "  [ 0.68918663]\n",
      "  [-3.6539469 ]]\n",
      "\n",
      " [[ 0.98761314]\n",
      "  [-0.05946481]\n",
      "  [-0.21467207]]\n",
      "\n",
      " [[ ...\n",
      "  [-0.49049693]]\n",
      "\n",
      " [[ 0.6714237 ]\n",
      "  [ 1.1756382 ]\n",
      "  [ 1.2150986 ]]\n",
      "\n",
      " [[ 0.9603348 ]\n",
      "  [-1.6555276 ]\n",
      "  [ 0.5345011 ]]])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m, out_grad: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, node: \u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Union[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, Tuple[\u001b[33m\"\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Compute partial adjoint for each input value for a given output adjoint.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Parameters\u001b[39;49;00m\n",
      "    \u001b[33m    ----------\u001b[39;49;00m\n",
      "    \u001b[33m    out_grad: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The adjoint wrt to the output value.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    node: Value\u001b[39;49;00m\n",
      "    \u001b[33m        The value node of forward evaluation.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns\u001b[39;49;00m\n",
      "    \u001b[33m    -------\u001b[39;49;00m\n",
      "    \u001b[33m    input_grads: Value or Tuple[Value]\u001b[39;49;00m\n",
      "    \u001b[33m        A list containing partial gradient adjoints to be propagated to\u001b[39;49;00m\n",
      "    \u001b[33m        each of the input node.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "node       = needle.Tensor([[[-1.2294166 ]\n",
      "  [ 0.68918663]\n",
      "  [-3.6539469 ]]\n",
      "\n",
      " [[ 0.98761314]\n",
      "  [-0.05946481]\n",
      "  [-0.21467207]]\n",
      "\n",
      " [[ ...\n",
      "  [-0.49049693]]\n",
      "\n",
      " [[ 0.6714237 ]\n",
      "  [ 1.1756382 ]\n",
      "  [ 1.2150986 ]]\n",
      "\n",
      " [[ 0.9603348 ]\n",
      "  [-1.6555276 ]\n",
      "  [ 0.5345011 ]]])\n",
      "out_grad   = needle.Tensor([[[ 1.2728198 ]\n",
      "  [ 1.1000712 ]\n",
      "  [-0.74093074]]\n",
      "\n",
      " [[-0.50888014]\n",
      "  [ 0.408108  ]\n",
      "  [ 0.53634965]]\n",
      "\n",
      " [[-...\n",
      "  [-0.22966217]]\n",
      "\n",
      " [[-1.2457664 ]\n",
      "  [ 0.1854648 ]\n",
      "  [-0.2667139 ]]\n",
      "\n",
      " [[-0.31800157]\n",
      "  [-1.2823775 ]\n",
      "  [ 1.4503455 ]]])\n",
      "self       = <needle.ops.ops_mathematic.Summation object at 0x7f86afbebc10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:63: NotImplementedError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes0-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 1), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[0.7175262]]])\n",
      "_A         = array([[[0.7175262]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:178: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[0.7175262]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.7175262]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1b51420>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[0.7175262]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1b51420>\n",
      "        tensor     = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1b52920>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1b52920>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, x, y)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[0.7175262]]], device=cpu())\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1b51420>\n",
      "        x          = 0\n",
      "        y          = 1\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:581: in swapaxes\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mswapaxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis1, axis2)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[0.7175262]]], device=cpu())\n",
      "        axis1      = 0\n",
      "        axis2      = 1\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:56: in _wrapfunc\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (0, 1)\n",
      "        bound      = None\n",
      "        kwds       = {}\n",
      "        method     = 'swapaxes'\n",
      "        obj        = NDArray([[[0.7175262]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[0.7175262]]], device=cpu()), method = 'swapaxes', args = (0, 1)\n",
      "kwds = {}, wrap = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapit\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = obj.__array_wrap__\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       result = \u001b[96mgetattr\u001b[39;49;00m(asarray(obj), method)(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dimension 0\u001b[0m\n",
      "\n",
      "args       = (0, 1)\n",
      "kwds       = {}\n",
      "method     = 'swapaxes'\n",
      "obj        = NDArray([[[0.7175262]]], device=cpu())\n",
      "wrap       = None\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:45: AxisError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes0-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 1), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.4280046...1.4355507   0.1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]])\n",
      "_A         = array([[[-1.1380908 ,  0.45943186,  0.8649256 , -0.5685719 ,\n",
      "         -0.13818243,  0.02258228],\n",
      "        [ 0.7159469 ,...29 ],\n",
      "        [-0.20496817, -0.13278429, -0.44021094,  0.8286478 ,\n",
      "          0.04491751,  1.9707588 ]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:178: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.4280046...1.4355507   0.1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.428004...4355507   0.1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1bbe1d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.428004...4355507   0.1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1bbe1d0>\n",
      "        tensor     = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1bbf490>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1bbf490>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, x, y)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.4280046   0.0...1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]], device=cpu())\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1bbe1d0>\n",
      "        x          = 0\n",
      "        y          = 1\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:581: in swapaxes\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mswapaxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis1, axis2)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.4280046   0.0...1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]], device=cpu())\n",
      "        axis1      = 0\n",
      "        axis2      = 1\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:56: in _wrapfunc\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (0, 1)\n",
      "        bound      = None\n",
      "        kwds       = {}\n",
      "        method     = 'swapaxes'\n",
      "        obj        = NDArray([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.4280046   0.0...1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.4280046   0.0...1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]], device=cpu())\n",
      "method = 'swapaxes', args = (0, 1), kwds = {}, wrap = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapit\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = obj.__array_wrap__\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       result = \u001b[96mgetattr\u001b[39;49;00m(asarray(obj), method)(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dimension 0\u001b[0m\n",
      "\n",
      "args       = (0, 1)\n",
      "kwds       = {}\n",
      "method     = 'swapaxes'\n",
      "obj        = NDArray([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.4280046   0.0...1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]], device=cpu())\n",
      "wrap       = None\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:45: AxisError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes1-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 2), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.63587433]]])\n",
      "_A         = array([[[-0.63587433]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:178: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.63587433]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.63587433]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1b717b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.63587433]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1b717b0>\n",
      "        tensor     = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1b730a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1b730a0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, x, y)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-0.63587433]]], device=cpu())\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1b717b0>\n",
      "        x          = 0\n",
      "        y          = 2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:581: in swapaxes\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mswapaxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis1, axis2)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-0.63587433]]], device=cpu())\n",
      "        axis1      = 0\n",
      "        axis2      = 2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:56: in _wrapfunc\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (0, 2)\n",
      "        bound      = None\n",
      "        kwds       = {}\n",
      "        method     = 'swapaxes'\n",
      "        obj        = NDArray([[[-0.63587433]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[-0.63587433]]], device=cpu()), method = 'swapaxes'\n",
      "args = (0, 2), kwds = {}, wrap = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapit\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = obj.__array_wrap__\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       result = \u001b[96mgetattr\u001b[39;49;00m(asarray(obj), method)(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dimension 0\u001b[0m\n",
      "\n",
      "args       = (0, 2)\n",
      "kwds       = {}\n",
      "method     = 'swapaxes'\n",
      "obj        = NDArray([[[-0.63587433]]], device=cpu())\n",
      "wrap       = None\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:45: AxisError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes1-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 2), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536 ...0.42731738 -0.6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]])\n",
      "_A         = array([[[-1.3012853 , -0.74705434,  1.2911866 ,  0.6756286 ,\n",
      "         -0.724772  ,  1.0151856 ],\n",
      "        [ 0.65764105,...04 ],\n",
      "        [-1.3640325 , -0.79039156,  0.4250712 ,  1.8339677 ,\n",
      "         -0.00649781,  0.58754677]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:178: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536 ...0.42731738 -0.6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536...42731738 -0.6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1cb0b50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536...42731738 -0.6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1cb0b50>\n",
      "        tensor     = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1cb3160>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1cb3160>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, x, y)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536 -0.294...6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]], device=cpu())\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1cb0b50>\n",
      "        x          = 0\n",
      "        y          = 2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:581: in swapaxes\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mswapaxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis1, axis2)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536 -0.294...6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]], device=cpu())\n",
      "        axis1      = 0\n",
      "        axis2      = 2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:56: in _wrapfunc\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (0, 2)\n",
      "        bound      = None\n",
      "        kwds       = {}\n",
      "        method     = 'swapaxes'\n",
      "        obj        = NDArray([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536 -0.294...6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536 -0.294...6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]], device=cpu())\n",
      "method = 'swapaxes', args = (0, 2), kwds = {}, wrap = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapit\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = obj.__array_wrap__\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       result = \u001b[96mgetattr\u001b[39;49;00m(asarray(obj), method)(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dimension 0\u001b[0m\n",
      "\n",
      "args       = (0, 2)\n",
      "kwds       = {}\n",
      "method     = 'swapaxes'\n",
      "obj        = NDArray([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536 -0.294...6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]], device=cpu())\n",
      "wrap       = None\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:45: AxisError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-None-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.8915784]]])\n",
      "_A         = array([[[-0.8915784]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:178: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.8915784]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.8915784]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1abbeb0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.8915784]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1abbeb0>\n",
      "        tensor     = <[AxisError(-1, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1abbd90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AxisError(-1, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1abbd90>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, x, y)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-0.8915784]]], device=cpu())\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1abbeb0>\n",
      "        x          = -1\n",
      "        y          = -2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:581: in swapaxes\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mswapaxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis1, axis2)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-0.8915784]]], device=cpu())\n",
      "        axis1      = -1\n",
      "        axis2      = -2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:56: in _wrapfunc\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (-1, -2)\n",
      "        bound      = None\n",
      "        kwds       = {}\n",
      "        method     = 'swapaxes'\n",
      "        obj        = NDArray([[[-0.8915784]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[-0.8915784]]], device=cpu()), method = 'swapaxes'\n",
      "args = (-1, -2), kwds = {}, wrap = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapit\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = obj.__array_wrap__\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       result = \u001b[96mgetattr\u001b[39;49;00m(asarray(obj), method)(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       numpy.exceptions.AxisError: axis1: axis -1 is out of bounds for array of dimension 0\u001b[0m\n",
      "\n",
      "args       = (-1, -2)\n",
      "kwds       = {}\n",
      "method     = 'swapaxes'\n",
      "obj        = NDArray([[[-0.8915784]]], device=cpu())\n",
      "wrap       = None\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:45: AxisError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-None-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768  ....5810264   0.57262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]])\n",
      "_A         = array([[[-0.23002987,  0.42615405,  0.42854363,  0.02079353,\n",
      "         -0.598037  , -0.30859315],\n",
      "        [ 1.2798208 ,...38 ],\n",
      "        [ 0.8136079 ,  1.3184868 ,  0.17044292,  0.13645671,\n",
      "         -0.42629206, -0.19551516]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:178: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768  ....5810264   0.57262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768 ...810264   0.57262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86afa151b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768 ...810264   0.57262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f86afa151b0>\n",
      "        tensor     = <[AxisError(-1, 0, 'axis1') raised in repr()] Tensor object at 0x7f86afa16170>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AxisError(-1, 0, 'axis1') raised in repr()] Tensor object at 0x7f86afa16170>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, x, y)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768  -1.343...7262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]], device=cpu())\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86afa151b0>\n",
      "        x          = -1\n",
      "        y          = -2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:581: in swapaxes\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mswapaxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis1, axis2)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768  -1.343...7262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]], device=cpu())\n",
      "        axis1      = -1\n",
      "        axis2      = -2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:56: in _wrapfunc\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (-1, -2)\n",
      "        bound      = None\n",
      "        kwds       = {}\n",
      "        method     = 'swapaxes'\n",
      "        obj        = NDArray([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768  -1.343...7262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768  -1.343...7262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]], device=cpu())\n",
      "method = 'swapaxes', args = (-1, -2), kwds = {}, wrap = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapit\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = obj.__array_wrap__\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       result = \u001b[96mgetattr\u001b[39;49;00m(asarray(obj), method)(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       numpy.exceptions.AxisError: axis1: axis -1 is out of bounds for array of dimension 0\u001b[0m\n",
      "\n",
      "args       = (-1, -2)\n",
      "kwds       = {}\n",
      "method     = 'swapaxes'\n",
      "obj        = NDArray([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768  -1.343...7262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]], device=cpu())\n",
      "wrap       = None\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:45: AxisError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes0-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[0.06534544]]])\n",
      "_A         = array([[[0.06534544]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:178: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[0.06534544]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.06534544]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86af5b3310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[0.06534544]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f86af5b3310>\n",
      "        tensor     = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86af5b37c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86af5b37c0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, x, y)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[0.06534544]]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86af5b3310>\n",
      "        x          = 0\n",
      "        y          = 1\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:581: in swapaxes\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mswapaxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis1, axis2)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[0.06534544]]], device=cuda())\n",
      "        axis1      = 0\n",
      "        axis2      = 1\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:56: in _wrapfunc\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (0, 1)\n",
      "        bound      = None\n",
      "        kwds       = {}\n",
      "        method     = 'swapaxes'\n",
      "        obj        = NDArray([[[0.06534544]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[0.06534544]]], device=cuda()), method = 'swapaxes'\n",
      "args = (0, 1), kwds = {}, wrap = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapit\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = obj.__array_wrap__\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       result = \u001b[96mgetattr\u001b[39;49;00m(asarray(obj), method)(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dimension 0\u001b[0m\n",
      "\n",
      "args       = (0, 1)\n",
      "kwds       = {}\n",
      "method     = 'swapaxes'\n",
      "obj        = NDArray([[[0.06534544]]], device=cuda())\n",
      "wrap       = None\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:45: AxisError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes0-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.9888552...1.3010299  -0.60691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]])\n",
      "_A         = array([[[-0.91022843,  0.64738804, -1.7764558 ,  1.5134803 ,\n",
      "          0.69402885,  0.09445285],\n",
      "        [ 1.1275321 ,...86 ],\n",
      "        [ 0.8013789 ,  0.98249215,  0.06927756,  2.1394844 ,\n",
      "          1.3729885 , -1.2533095 ]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:178: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.9888552...1.3010299  -0.60691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.988855...3010299  -0.60691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1bb9e10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.988855...3010299  -0.60691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1bb9e10>\n",
      "        tensor     = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1bbbb50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1bbbb50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, x, y)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.98885524  0.1...0691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1bb9e10>\n",
      "        x          = 0\n",
      "        y          = 1\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:581: in swapaxes\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mswapaxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis1, axis2)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.98885524  0.1...0691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]], device=cuda())\n",
      "        axis1      = 0\n",
      "        axis2      = 1\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:56: in _wrapfunc\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (0, 1)\n",
      "        bound      = None\n",
      "        kwds       = {}\n",
      "        method     = 'swapaxes'\n",
      "        obj        = NDArray([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.98885524  0.1...0691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.98885524  0.1...0691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]], device=cuda())\n",
      "method = 'swapaxes', args = (0, 1), kwds = {}, wrap = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapit\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = obj.__array_wrap__\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       result = \u001b[96mgetattr\u001b[39;49;00m(asarray(obj), method)(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dimension 0\u001b[0m\n",
      "\n",
      "args       = (0, 1)\n",
      "kwds       = {}\n",
      "method     = 'swapaxes'\n",
      "obj        = NDArray([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.98885524  0.1...0691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]], device=cuda())\n",
      "wrap       = None\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:45: AxisError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes1-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 2), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.69669217]]])\n",
      "_A         = array([[[-0.69669217]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:178: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.69669217]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.69669217]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1bc65f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.69669217]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1bc65f0>\n",
      "        tensor     = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1bc6da0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1bc6da0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, x, y)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-0.69669217]]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1bc65f0>\n",
      "        x          = 0\n",
      "        y          = 2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:581: in swapaxes\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mswapaxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis1, axis2)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-0.69669217]]], device=cuda())\n",
      "        axis1      = 0\n",
      "        axis2      = 2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:56: in _wrapfunc\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (0, 2)\n",
      "        bound      = None\n",
      "        kwds       = {}\n",
      "        method     = 'swapaxes'\n",
      "        obj        = NDArray([[[-0.69669217]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[-0.69669217]]], device=cuda()), method = 'swapaxes'\n",
      "args = (0, 2), kwds = {}, wrap = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapit\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = obj.__array_wrap__\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       result = \u001b[96mgetattr\u001b[39;49;00m(asarray(obj), method)(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dimension 0\u001b[0m\n",
      "\n",
      "args       = (0, 2)\n",
      "kwds       = {}\n",
      "method     = 'swapaxes'\n",
      "obj        = NDArray([[[-0.69669217]]], device=cuda())\n",
      "wrap       = None\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:45: AxisError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes1-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 2), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311....9787657   0.12266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]])\n",
      "_A         = array([[[-0.93575615, -0.61421335, -0.45289811,  0.96114016,\n",
      "          0.23405291, -0.8781786 ],\n",
      "        [-0.40377602,...342],\n",
      "        [ 1.1850599 , -0.7730635 ,  1.5469892 , -0.71304744,\n",
      "          0.92298496, -0.52088904]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:178: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311....9787657   0.12266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.361931...787657   0.12266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1c32200>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.361931...787657   0.12266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1c32200>\n",
      "        tensor     = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1c33610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AxisError(0, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1c33610>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, x, y)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311  -0.0...266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1c32200>\n",
      "        x          = 0\n",
      "        y          = 2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:581: in swapaxes\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mswapaxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis1, axis2)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311  -0.0...266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]], device=cuda())\n",
      "        axis1      = 0\n",
      "        axis2      = 2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:56: in _wrapfunc\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (0, 2)\n",
      "        bound      = None\n",
      "        kwds       = {}\n",
      "        method     = 'swapaxes'\n",
      "        obj        = NDArray([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311  -0.0...266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311  -0.0...266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]], device=cuda())\n",
      "method = 'swapaxes', args = (0, 2), kwds = {}, wrap = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapit\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = obj.__array_wrap__\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       result = \u001b[96mgetattr\u001b[39;49;00m(asarray(obj), method)(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dimension 0\u001b[0m\n",
      "\n",
      "args       = (0, 2)\n",
      "kwds       = {}\n",
      "method     = 'swapaxes'\n",
      "obj        = NDArray([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311  -0.0...266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]], device=cuda())\n",
      "wrap       = None\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:45: AxisError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cuda-None-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[0.03809374]]])\n",
      "_A         = array([[[0.03809374]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:178: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[0.03809374]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.03809374]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1be7af0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[0.03809374]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1be7af0>\n",
      "        tensor     = <[AxisError(-1, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1be64d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AxisError(-1, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1be64d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, x, y)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[0.03809374]]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1be7af0>\n",
      "        x          = -1\n",
      "        y          = -2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:581: in swapaxes\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mswapaxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis1, axis2)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[0.03809374]]], device=cuda())\n",
      "        axis1      = -1\n",
      "        axis2      = -2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:56: in _wrapfunc\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (-1, -2)\n",
      "        bound      = None\n",
      "        kwds       = {}\n",
      "        method     = 'swapaxes'\n",
      "        obj        = NDArray([[[0.03809374]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[0.03809374]]], device=cuda()), method = 'swapaxes'\n",
      "args = (-1, -2), kwds = {}, wrap = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapit\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = obj.__array_wrap__\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       result = \u001b[96mgetattr\u001b[39;49;00m(asarray(obj), method)(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       numpy.exceptions.AxisError: axis1: axis -1 is out of bounds for array of dimension 0\u001b[0m\n",
      "\n",
      "args       = (-1, -2)\n",
      "kwds       = {}\n",
      "method     = 'swapaxes'\n",
      "obj        = NDArray([[[0.03809374]]], device=cuda())\n",
      "wrap       = None\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:45: AxisError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cuda-None-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.0556547...1.862709   -0.3764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]])\n",
      "_A         = array([[[ 0.9778864 ,  0.07981496, -1.6533154 , -0.28263307,\n",
      "         -0.51364565,  0.08389966],\n",
      "        [-0.5758165 ,...7  ],\n",
      "        [ 0.11819558,  0.84113216, -0.40530366, -0.4401012 ,\n",
      "         -0.16526793, -0.31185058]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:178: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.0556547...1.862709   -0.3764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.055654...862709   -0.3764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1c39750>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.055654...862709   -0.3764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1c39750>\n",
      "        tensor     = <[AxisError(-1, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1c39600>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AxisError(-1, 0, 'axis1') raised in repr()] Tensor object at 0x7f86a1c39600>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, x, y)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.05565479 -0.7...764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]], device=cuda())\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f86a1c39750>\n",
      "        x          = -1\n",
      "        y          = -2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:581: in swapaxes\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mswapaxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis1, axis2)\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.05565479 -0.7...764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]], device=cuda())\n",
      "        axis1      = -1\n",
      "        axis2      = -2\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:56: in _wrapfunc\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (-1, -2)\n",
      "        bound      = None\n",
      "        kwds       = {}\n",
      "        method     = 'swapaxes'\n",
      "        obj        = NDArray([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.05565479 -0.7...764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.05565479 -0.7...764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]], device=cuda())\n",
      "method = 'swapaxes', args = (-1, -2), kwds = {}, wrap = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapit\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = obj.__array_wrap__\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            wrap = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       result = \u001b[96mgetattr\u001b[39;49;00m(asarray(obj), method)(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       numpy.exceptions.AxisError: axis1: axis -1 is out of bounds for array of dimension 0\u001b[0m\n",
      "\n",
      "args       = (-1, -2)\n",
      "kwds       = {}\n",
      "method     = 'swapaxes'\n",
      "obj        = NDArray([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.05565479 -0.7...764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]], device=cuda())\n",
      "wrap       = None\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:45: AxisError\n",
      "\u001b[31m\u001b[1m_______________________ test_logsumexp[cpu-shape0-None] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[0.26994687]]])\n",
      "A_t        = tensor([[[0.2699]]])\n",
      "_A         = array([[[0.26994687]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "t_axes     = (0, 1, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:66: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[0.26994687]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.26994687]]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86afa15f60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[0.26994687]]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86afa15f60>\n",
      "        tensor     = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86afa15360>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86afa15360>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:34: in compute\n",
      "    \u001b[0mdata = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[[0.26994687]]], device=cpu())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86afa15f60>\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:2810: in max\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[0.26994687]]], device=cpu())\n",
      "        axis       = None\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[0.26994687]]], device=cpu()), ufunc = <ufunc 'maximum'>\n",
      "method = 'max', axis = None, dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[[0.26994687]]], device=cpu())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: NDArray.max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = None\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[[0.26994687]]], device=cpu())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[[0.26994687]]], device=cpu())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:86: TypeError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]])\n",
      "A_t        = tensor([[ 2.0157,  0.2940,  1.3641],\n",
      "        [ 1.5222,  0.7832, -0.1521],\n",
      "        [ 1.6932,  1.2563,  0.1421],\n",
      "        [ 1.7393,  0.4600, -1.6940],\n",
      "        [-1.1900, -1.3298,  1.6369]])\n",
      "_A         = array([[ 2.0156775 ,  0.29399768,  1.3641229 ],\n",
      "       [ 1.522246  ,  0.7832116 , -0.15208119],\n",
      "       [ 1.6932101 ,  ...1299 ],\n",
      "       [ 1.7393255 ,  0.46002465, -1.6939654 ],\n",
      "       [-1.1899973 , -1.3297868 ,  1.6368964 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cpu()\n",
      "shape      = (5, 3)\n",
      "t_axes     = 0\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:66: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1b9d660>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1b9d660>\n",
      "        tensor     = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86a1b9d720>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86a1b9d720>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:34: in compute\n",
      "    \u001b[0mdata = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]], device=cpu())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1b9d660>\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:2810: in max\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]], device=cpu())\n",
      "        axis       = (0,)\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]], device=cpu())\n",
      "ufunc = <ufunc 'maximum'>, method = 'max', axis = (0,), dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ ... 1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]], device=cpu())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: NDArray.max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = (0,)\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]], device=cpu())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ ... 1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]], device=cpu())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:86: TypeError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1....8]\n",
      "  [ 0.66724986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]])\n",
      "A_t        = tensor([[[-1.4058, -1.3156],\n",
      "         [-0.8430, -0.4802],\n",
      "         [-0.4784,  0.9575]],\n",
      "\n",
      "        [[-1.0233, -1.1449],\n",
      "...         [ 0.6672, -0.0410]],\n",
      "\n",
      "        [[-1.6006, -0.4157],\n",
      "         [ 1.3384,  0.9067],\n",
      "         [ 0.1184, -0.3027]]])\n",
      "_A         = array([[[-1.4058412 , -1.3155643 ],\n",
      "        [-0.8429701 , -0.48019046],\n",
      "        [-0.47839305,  0.95748687]],\n",
      "\n",
      "       [...  [[-1.6006118 , -0.41570628],\n",
      "        [ 1.3384075 ,  0.9066571 ],\n",
      "        [ 0.11837886, -0.30268484]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:66: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1....8]\n",
      "  [ 0.66724986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1...\n",
      "  [ 0.66724986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bc3190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1...\n",
      "  [ 0.66724986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bc3190>\n",
      "        tensor     = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86a1bc25f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86a1bc25f0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:34: in compute\n",
      "    \u001b[0mdata = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1.144900...986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]], device=cpu())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bc3190>\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:2810: in max\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1.144900...986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]], device=cpu())\n",
      "        axis       = (1,)\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1.144900...986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]], device=cpu())\n",
      "ufunc = <ufunc 'maximum'>, method = 'max', axis = (1,), dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.9574868...86 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]], device=cpu())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: NDArray.max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = (1,)\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1.144900...986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]], device=cpu())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.9574868...86 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]], device=cpu())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:86: TypeError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1.... ]\n",
      "  [-0.8324522  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]])\n",
      "A_t        = tensor([[[ 0.6371, -0.1442],\n",
      "         [-1.1688, -0.5101],\n",
      "         [-2.6889, -0.6259]],\n",
      "\n",
      "        [[-1.3568,  1.1981],\n",
      "...         [-0.8325, -0.3871]],\n",
      "\n",
      "        [[-0.4447, -0.2915],\n",
      "         [ 2.9369,  0.4590],\n",
      "         [ 0.8437, -0.1112]]])\n",
      "_A         = array([[[ 0.63712746, -0.14424577],\n",
      "        [-1.1687807 , -0.5100969 ],\n",
      "        [-2.688898  , -0.62585706]],\n",
      "\n",
      "       [...  [[-0.4446728 , -0.29152763],\n",
      "        [ 2.936914  ,  0.4590318 ],\n",
      "        [ 0.8436789 , -0.1111581 ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:66: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1.... ]\n",
      "  [-0.8324522  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1...\n",
      "  [-0.8324522  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bd6500>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1...\n",
      "  [-0.8324522  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bd6500>\n",
      "        tensor     = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86a1bd4d60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86a1bd4d60>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:34: in compute\n",
      "    \u001b[0mdata = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1.198088...22  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]], device=cpu())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bd6500>\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:2810: in max\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1.198088...22  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]], device=cpu())\n",
      "        axis       = (2,)\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1.198088...22  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]], device=cpu())\n",
      "ufunc = <ufunc 'maximum'>, method = 'max', axis = (2,), dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.6258570...2  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]], device=cpu())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: NDArray.max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = (2,)\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1.198088...22  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]], device=cpu())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.6258570...2  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]], device=cpu())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:86: TypeError\n",
      "\u001b[31m\u001b[1m_______________________ test_logsumexp[cuda-shape0-None] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[1.3820337]]])\n",
      "A_t        = tensor([[[1.3820]]])\n",
      "_A         = array([[[1.3820337]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "t_axes     = (0, 1, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:66: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[1.3820337]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[1.3820337]]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bba5f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[1.3820337]]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bba5f0>\n",
      "        tensor     = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86a1bb85b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86a1bb85b0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:34: in compute\n",
      "    \u001b[0mdata = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[[1.3820337]]], device=cuda())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bba5f0>\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:2810: in max\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[1.3820337]]], device=cuda())\n",
      "        axis       = None\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[1.3820337]]], device=cuda()), ufunc = <ufunc 'maximum'>\n",
      "method = 'max', axis = None, dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[[1.3820337]]], device=cuda())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: NDArray.max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = None\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[[1.3820337]]], device=cuda())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[[1.3820337]]], device=cuda())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:86: TypeError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]])\n",
      "A_t        = tensor([[ 0.5095, -0.1391, -0.4883],\n",
      "        [-1.4052,  0.2294, -0.4997],\n",
      "        [ 0.0422, -1.1866, -0.4498],\n",
      "        [-0.4322,  0.6269, -1.8056],\n",
      "        [ 0.6719, -0.0803,  1.6619]])\n",
      "_A         = array([[ 0.5095096 , -0.13911654, -0.48825276],\n",
      "       [-1.4051776 ,  0.22937068, -0.49968913],\n",
      "       [ 0.042238  , -...7797 ],\n",
      "       [-0.432245  ,  0.6268619 , -1.8056363 ],\n",
      "       [ 0.67194265, -0.08028104,  1.6619049 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cuda()\n",
      "shape      = (5, 3)\n",
      "t_axes     = 0\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:66: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1b822c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1b822c0>\n",
      "        tensor     = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86a1b82e60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86a1b82e60>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:34: in compute\n",
      "    \u001b[0mdata = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1b822c0>\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:2810: in max\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]], device=cuda())\n",
      "        axis       = (0,)\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]], device=cuda())\n",
      "ufunc = <ufunc 'maximum'>, method = 'max', axis = (0,), dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ ...1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]], device=cuda())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: NDArray.max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = (0,)\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]], device=cuda())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ ...1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]], device=cuda())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:86: TypeError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e-...5e-01]]\n",
      "\n",
      " [[-7.08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]])\n",
      "A_t        = tensor([[[-9.0702e-01, -3.6689e-01],\n",
      "         [ 3.3903e-01, -8.2990e-01],\n",
      "         [ 2.3796e-01,  3.0322e-01]],\n",
      "\n",
      "     ...01]],\n",
      "\n",
      "        [[-7.0857e-01, -7.2344e-01],\n",
      "         [ 1.0095e-01, -4.2828e-01],\n",
      "         [ 1.3839e+00, -5.5759e-01]]])\n",
      "_A         = array([[[-9.07018244e-01, -3.66889060e-01],\n",
      "        [ 3.39028060e-01, -8.29904377e-01],\n",
      "        [ 2.37964749e-01,  3.0...23435521e-01],\n",
      "        [ 1.00946397e-01, -4.28279608e-01],\n",
      "        [ 1.38385820e+00, -5.57593405e-01]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:66: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e-...5e-01]]\n",
      "\n",
      " [[-7.08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e...-01]]\n",
      "\n",
      " [[-7.08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bd7fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e...-01]]\n",
      "\n",
      " [[-7.08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bd7fd0>\n",
      "        tensor     = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86a1bd7af0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86a1bd7af0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:34: in compute\n",
      "    \u001b[0mdata = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e-01]]\n",
      "\n",
      "...08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]], device=cuda())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bd7fd0>\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:2810: in max\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e-01]]\n",
      "\n",
      "...08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]], device=cuda())\n",
      "        axis       = (1,)\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e-01]]\n",
      "\n",
      "...08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]], device=cuda())\n",
      "ufunc = <ufunc 'maximum'>, method = 'max', axis = (1,), dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.379...8571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]], device=cuda())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: NDArray.max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = (1,)\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e-01]]\n",
      "\n",
      "...08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]], device=cuda())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.379...8571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]], device=cuda())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:86: TypeError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1.... ]\n",
      "  [-2.032398   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]])\n",
      "A_t        = tensor([[[ 1.1967, -1.5918],\n",
      "         [-0.4074,  2.0170],\n",
      "         [ 0.3955, -0.6189]],\n",
      "\n",
      "        [[ 1.0398, -1.0497],\n",
      "...         [-2.0324, -0.6086]],\n",
      "\n",
      "        [[-1.9792,  0.4210],\n",
      "         [-0.6725, -0.5483],\n",
      "         [ 0.2719, -1.1673]]])\n",
      "_A         = array([[[ 1.196672  , -1.5917976 ],\n",
      "        [-0.40744546,  2.01703   ],\n",
      "        [ 0.3955127 , -0.6189291 ]],\n",
      "\n",
      "       [...  [[-1.9791524 ,  0.42104447],\n",
      "        [-0.67249846, -0.5482779 ],\n",
      "        [ 0.2719008 , -1.167323  ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:66: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1.... ]\n",
      "  [-2.032398   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1...\n",
      "  [-2.032398   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bf0850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1...\n",
      "  [-2.032398   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bf0850>\n",
      "        tensor     = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86a1bf02e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"NDArray.max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f86a1bf02e0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:34: in compute\n",
      "    \u001b[0mdata = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1.049749...   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]], device=cuda())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f86a1bf0850>\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:2810: in max\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1.049749...   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]], device=cuda())\n",
      "        axis       = (2,)\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1.049749...   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]], device=cuda())\n",
      "ufunc = <ufunc 'maximum'>, method = 'max', axis = (2,), dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291...  -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]], device=cuda())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: NDArray.max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = (2,)\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1.049749...   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]], device=cuda())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291...  -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]], device=cuda())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../../../anaconda3/envs/taichi/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:86: TypeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_relu[cpu-shape0]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_relu[cpu-shape1]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_relu[cuda-shape1]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh[cpu-shape0]\u001b[0m - AttributeError: module 'needle' has no attribute 'tanh'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh[cpu-shape1]\u001b[0m - AttributeError: module 'needle' has no attribute 'tanh'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh[cuda-shape0]\u001b[0m - AttributeError: module 'needle' has no attribute 'tanh'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh[cuda-shape1]\u001b[0m - AttributeError: module 'needle' has no attribute 'tanh'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh_backward[cpu-shape0]\u001b[0m - AttributeError: module 'needle' has no attribute 'tanh'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh_backward[cpu-shape1]\u001b[0m - AttributeError: module 'needle' has no attribute 'tanh'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh_backward[cuda-shape0]\u001b[0m - AttributeError: module 'needle' has no attribute 'tanh'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh_backward[cuda-shape1]\u001b[0m - AttributeError: module 'needle' has no attribute 'tanh'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape0-0-1]\u001b[0m - AttributeError: module 'needle' has no attribute 'stack'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape1-0-2]\u001b[0m - AttributeError: module 'needle' has no attribute 'stack'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape2-2-5]\u001b[0m - AttributeError: module 'needle' has no attribute 'stack'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cuda-shape0-0-1]\u001b[0m - AttributeError: module 'needle' has no attribute 'stack'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cuda-shape1-0-2]\u001b[0m - AttributeError: module 'needle' has no attribute 'stack'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cuda-shape2-2-5]\u001b[0m - AttributeError: module 'needle' has no attribute 'stack'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape0-0-1]\u001b[0m - AttributeError: module 'needle' has no attribute 'stack'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape1-0-2]\u001b[0m - AttributeError: module 'needle' has no attribute 'stack'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape2-2-5]\u001b[0m - AttributeError: module 'needle' has no attribute 'stack'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape0-0-1]\u001b[0m - AttributeError: module 'needle' has no attribute 'stack'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape1-0-2]\u001b[0m - AttributeError: module 'needle' has no attribute 'stack'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape2-2-5]\u001b[0m - AttributeError: module 'needle' has no attribute 'stack'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cpu-shape1-0]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cpu-shape2-1]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cpu-shape3-2]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cuda-shape1-0]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cuda-shape2-1]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cuda-shape3-2]\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape0-None]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape2-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape0-None]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape1-0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape2-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape3-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes0-shape0]\u001b[0m - numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dim...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes0-shape1]\u001b[0m - numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dim...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes1-shape0]\u001b[0m - numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dim...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes1-shape1]\u001b[0m - numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dim...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-None-shape0]\u001b[0m - numpy.exceptions.AxisError: axis1: axis -1 is out of bounds for array of di...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-None-shape1]\u001b[0m - numpy.exceptions.AxisError: axis1: axis -1 is out of bounds for array of di...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes0-shape0]\u001b[0m - numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dim...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes0-shape1]\u001b[0m - numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dim...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes1-shape0]\u001b[0m - numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dim...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes1-shape1]\u001b[0m - numpy.exceptions.AxisError: axis1: axis 0 is out of bounds for array of dim...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-None-shape0]\u001b[0m - numpy.exceptions.AxisError: axis1: axis -1 is out of bounds for array of di...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-None-shape1]\u001b[0m - numpy.exceptions.AxisError: axis1: axis -1 is out of bounds for array of di...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape0-None]\u001b[0m - TypeError: NDArray.max() got an unexpected keyword argument 'out'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape1-0]\u001b[0m - TypeError: NDArray.max() got an unexpected keyword argument 'out'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape2-1]\u001b[0m - TypeError: NDArray.max() got an unexpected keyword argument 'out'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape3-2]\u001b[0m - TypeError: NDArray.max() got an unexpected keyword argument 'out'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape0-None]\u001b[0m - TypeError: NDArray.max() got an unexpected keyword argument 'out'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape1-0]\u001b[0m - TypeError: NDArray.max() got an unexpected keyword argument 'out'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape2-1]\u001b[0m - TypeError: NDArray.max() got an unexpected keyword argument 'out'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape3-2]\u001b[0m - TypeError: NDArray.max() got an unexpected keyword argument 'out'\n",
      "\u001b[31m================ \u001b[31m\u001b[1m57 failed\u001b[0m, \u001b[32m61 passed\u001b[0m, \u001b[33m1685 deselected\u001b[0m\u001b[31m in 4.27s\u001b[0m\u001b[31m ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"nd_backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"new_nd_backend\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CIFAR-10 dataset [10 points]\n",
    "\n",
    "Next, you will write support for the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image classification dataset, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50k training images and 10k test images. \n",
    "\n",
    "Start by implementing the `__init__` function in the `CIFAR10Dataset` class in `python/needle/data/datasets/cifar10_dataset.py`. You can read in the link above how to properly read the CIFAR-10 dataset files you downloaded at the beginning of the homework. Also fill in `__getitem__` and `__len__`. Note that the return shape of the data from `__getitem__` should be in order (3, 32, 32).\n",
    "\n",
    "Copy `python/needle/data/data_transforms.py` and `python/needle/data/data_basic.py` from previous homeworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"test_cifar10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3: Convolutional neural network [40 points]\n",
    "\n",
    "Here's an outline of what you will do in this task.\n",
    "\n",
    "In `python/needle/backend_ndarray/ndarray.py`, implement:\n",
    "- `flip`\n",
    "- `pad`\n",
    "\n",
    "In `python/needle/ops_mathematic.py`, implement (forward and backward):\n",
    "- `Flip`\n",
    "- `Dilate`\n",
    "- `UnDilate`\n",
    "- `Conv`\n",
    "\n",
    "In `python/needle/nn/nn_conv.py`, implement:\n",
    "- `Conv`\n",
    "\n",
    "In `apps/models.py`, fill in the `ResNet9` class.  \n",
    "\n",
    "In `apps/simple_ml.py`, fill in:\n",
    "- `epoch_general_cifar10`,\n",
    "- `train_cifar10`\n",
    "- `evaluate_cifar10`\n",
    "\n",
    "We have provided a `BatchNorm2d` implementation in `python/needle/nn/nn_basic.py` for you as a wrapper around your previous `BatchNorm1d` implementation. \n",
    "\n",
    "**Note**: Remember to copy the solution of `nn_basic.py` from previous homework, make sure to not overwrite the `BatchNorm2d` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding ndarrays\n",
    "\n",
    "Convolution as typically implemented in deep learning libraries cuts down the size of inputs;\n",
    "e.g., a (1, 32, 32, 3) image convolved with a 3x3 filter would give a (1, 30, 30, c) output.\n",
    "A way around this is to pad the input ndarray before performing convolution, e.g., pad with zeros to get a (1, 34, 34, 3) ndarray so that the result is (1, 32, 32, 3). \n",
    "\n",
    "Padding is also required for the backward pass of convolution.\n",
    "\n",
    "You should implement `pad` in `ndarray.py` to closely reflect the behavior of `np.pad`.\n",
    "That is, `pad` should take a tuple of 2-tuples with length equal to the number of dimensions of the array,\n",
    "where each element in the 2-tuple corresponds to \"left padding\" and \"right padding\", respectively.\n",
    "\n",
    "For example, if `A` is a (10, 32, 32, 8) ndarray (think NHWC), then `A.pad( (0, 0), (2, 2), (2, 2), (0, 0) )` would be a (10, 36, 36, 8) ndarray where the \"spatial\" dimension has been padded by two zeros on all sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"pad_forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flipping ndarrays & FlipOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility code for a demonstration below which you can probably ignore. It might be instructive to check out the `offset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads off the underlying data array in order (i.e., offset 0, offset 1, ..., offset n)\n",
    "# i.e., ignoring strides\n",
    "def raw_data(X):\n",
    "    X = np.array(X) # copy, thus compact X\n",
    "    return np.frombuffer(ctypes.string_at(X.ctypes.data, X.nbytes), dtype=X.dtype, count=X.size)\n",
    "\n",
    "# Xold and Xnew should reference the same underlying data\n",
    "def offset(Xold, Xnew):\n",
    "    assert Xold.itemsize == Xnew.itemsize\n",
    "    # compare addresses to the beginning of the arrays\n",
    "    return (Xnew.ctypes.data - Xold.ctypes.data)//Xnew.itemsize\n",
    "\n",
    "def strides(X):\n",
    "    return ', '.join([str(x//X.itemsize) for x in X.strides])\n",
    "\n",
    "def format_array(X, shape):\n",
    "    assert len(shape) == 3, \"I only made this formatting work for ndims = 3\"\n",
    "    def chunks(l, n):\n",
    "        n = max(1, n)\n",
    "        return (l[i:i+n] for i in range(0, len(l), n))\n",
    "    a = [str(x) if x >= 10 else ' ' + str(x) for x in X]\n",
    "    a = ['(' + ' '.join(y) + ')' for y in [x for x in chunks(a, shape[-1])]]\n",
    "    a = ['|' + ' '.join(y) + '|' for y in [x for x in chunks(a, shape[-2])]]\n",
    "    return '  '.join(a)\n",
    "\n",
    "def inspect_array(X, *, is_a_copy_of):\n",
    "    # compacts X, then reads it off in order\n",
    "    print('Data: %s' % format_array(raw_data(X), X.shape))\n",
    "    # compares address of X to copy_of, thus finding X's offset\n",
    "    print('Offset: %s' % offset(is_a_copy_of, X))\n",
    "    print('Strides: %s' % strides(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to implement the backwards pass of 2D convolution, we will (probably) need a function which _flips_\n",
    "axes of ndarrays. We say \"probably\" because you could probably cleverly implement your convolution forward\n",
    "function to avoid this. However, we think it is easiest to think about this if you have the ability to \"flip\" the kernel along its vertical and horizontal dimensions.\n",
    "\n",
    "We will try to build up your intuition for the \"flip\" operation below in order to help you figure out how to implement it in `ndarray.py`. To do that, we explore numpy's `np.flip` function below. One thing to note is that\n",
    "`flip` is typically implemented by using negative strides and changing the _offset_ of the underlying array.\n",
    "\n",
    "For example, flipping an array on _all_ of its axes is equivalent to reversing the array. In this case, you can imagine that we would want all the strides to be negative, and the offset to be the length of the array (to start at the end of the array and \"stride\" backwards).\n",
    "\n",
    "Since we did not explicitly support negative strides in our implementation for the last homework, we will merely call `NDArray.make` with them to make our \"flipped\" array and then immediately call `.compact()`. Other than changing unsigned ints to signed ints in a few places, we suspect your existing `compact` function should not have to change at all to accomodate negative strides. In the .cc and .cu files we distributed, we have already changed the function signatures to reflect this.\n",
    "\n",
    "Alternatively, you could simply implement `flip` in the CPU backend by copying memory, which you _may_ find more intuitive. We suggest following our mini tutorial below to keep your implementation Python-focused, since we believe it is involves approximately the same amount of effort to implement it slightly more naively in C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this array as reference for the other examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(1, 25).reshape(3, 2, 4)\n",
    "inspect_array(A, is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have put brackets around each axis of the array. Notice that for this array, the offset is 0 and the strides are all positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happens when you flip the array along the last axis below. \n",
    "Note that the `inspect_array` function compacts the array after flipping it so you can see the\n",
    "\"logical\" order of the data, and the offset is calculated by comparing the address of the **non**-compacted\n",
    "flipped array with that of `is_copy_of`, i.e., the array `A` we looked at above.\n",
    "\n",
    "That is, we are looking at how numpy calculates the strides and offset for flipped arrays in order\n",
    "to copy this behavior in our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (2,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So flipping the last axis reverses the order of the elements within each 4-dimensional \"cell\", as you can see above. The stride corresponding to the axis we flipped has been negated. And the offset is 3 -- this makes sense, e.g., because we want the new \"first\" element of the array to be 4, which was at index 3 in `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (1,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again for the middle axis: we negate the middle stride, and the offset is 4, which seems reasonable since we now want the first element to be 5, which was at index 4 in the original array `A`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to infer the more general algorithm for computing the offset given the axis to flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe what happens when we flip _all_ axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,1,2)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the offset is then sufficient to point to the last element of the array, and this is just the \"reverse order\" version of `A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we flip just axes 1 and 0..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,1)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offset is 20. Looking back on our previous offset computations, do you notice something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "With this exploration of numpy's ndarray flipping functionality, which uses negative strides and a custom offset,\n",
    "try to implement `flip` in `ndarray.py`. You also must implement \"flip\" forward and backward functions in `ops.py`; note that these should be extremely short.\n",
    "\n",
    "**Important:** You should call NDArray.make with the new strides and offset, and then immediately `.compact()` this array. The resulting array is then copied and has positive strides. We want this (less-than-optimal) behavior because we did not account for negative strides in our previous implementation. _Aside:_ If you want, consider where/if negative strides break your implementation. `__getitem__` definitely doesn't work due to how we processed slices; is there anything else? (_Note_: this isn't graded.)\n",
    "\n",
    "Also, if you want to instead add a `flip` operator on the CPU/CUDA backends, that's also okay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"flip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dilation operator puts zeros between elements of an ndarray. We will need it for computing the backward pass of convolution when the stride of the convolution is greater than 1. As an example, dilation should do the following to a 2x2 matrix when dilated by 1 on both axes:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "\\Longrightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "3 & 0 & 4 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To get some intuition for why we need dilation for the backward pass of strided convolution, consider a  `stride=2`, `padding=\"same\"`, `input_channels=output_channels=8` convolution applied to an input of size (10, 32, 32, 8). The resulting output will be of size (10, 16, 16, 8) due to the stride, and thus `out_grad` will have shape (10, 16, 16, 8). Yet, the gradient of the input needs to, of course, have shape (10, 32, 32, 8) -- so we must need to increase the size of `out_grad` in some way. Consider also that you could implement strided convolution as `Conv(x)[:, ::2, ::2, :]`, i.e., only keeping every other pixel in the spatial dimension.\n",
    "\n",
    "\n",
    "Implement `Dilate` in `ops.py`. This function takes two additional parameters (in attrs): the `dilation` amount and the `axes` to dilate. You must also implement the corresponding op `UnDilate`, whose forward pass will be used to implement the gradient of `Dilate`. (This is so we do not have to implement `GetItem` and `SetItem` ops, which can be highly inefficient to backprop through without additional optimizations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"dilate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit new ops (flip/dilation) to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"new_ops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution forward\n",
    "\n",
    "Implement the forward pass of 2D multi-channel convolution in `ops.py`. You should probably refer to [this notebook](https://github.com/dlsyscourse/public_notebooks/blob/main/convolution_implementation.ipynb) from lecture, which implements 2D multi-channel convolution using im2col in numpy.\n",
    "\n",
    "**Note:** Your convolution op should accept tensors in the NHWC format, as in the example above, and weights in the format (kernel_size, kernel_size, input_channels, output_channels).\n",
    "\n",
    "However, you will need to add two additional features. Your convolution function should accept arguments for `padding` (default 0) and `stride` (default 1). For `padding`, you should simply apply your padding function to the spatial dimensions (i.e., axes 1 and 2). \n",
    "\n",
    "Implementing strided convolution should consist of a relatively small set of changes to your plain convolution implementation.\n",
    "\n",
    "We recommend implementing convolution without stride first, ensuring you pass some of the tests below, and then adding in stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the gradients of 2D multi-channel convolution can be technically quite challenging (especially \"rigorously\"). We will try to provide some useful hints here. Basically, we encourage you to make use of the surprising fact that _whatever makes the dimensions work out is typically right_.\n",
    "\n",
    "Ultimately, the backward pass of convolution can be done in terms of the convolution operator itself, with some clever manipulations using `flip`, `dilate`, and multiple applications of `transpose` to both the arguments and the results.\n",
    "\n",
    "In the last section, we essentially implemented convolution as a matrix product: ignoring the various restride and reshape operations, we basically have something like `X @ W`, where `X` is the input and `W` is the weight. We also have `out_grad`, which is the same shape as `X @ W`. Now, you have already implemented the backward pass of matrix multiplication in a previous assignment, and we can use this knowledge to get some insight into the backward pass of convolution. In particular, referencing your matmul backward implementation, you may notice (heuristically speaking here):\n",
    "\n",
    "`X.grad = out_grad @ W.transpose` \\\n",
    "`W.grad = X.transpose @ out_grad`\n",
    "\n",
    "Surprisingly enough, things work out if we just assume that these are also convolutions (and now assuming that `out_grad`, `W`, and `X` are tensors amenable to 2D multi-channel convolution instead of matrices):\n",
    "\n",
    "`X.grad = ≈conv(≈out_grad, ≈W)` \\\n",
    "`W.grad = ≈conv(≈X, ≈out_grad)`\n",
    "\n",
    "In which the \"≈\" indicates that you need to apply some additional operators to these terms in order to get the dimensions to work out, such as permuting/transposing axes, dilating, changing the `padding=` argument to the convolution function, or permuting/transposing axes of the resulting convolution.\n",
    "\n",
    "As we saw on the [last few slides here](https://dlsyscourse.org/slides/conv_nets.pdf) in class, the transpose of a convolution can be found by simply flipping the kernel. Since we're working in 2D instead of 1D, this means flipping the kernel both vertically and horizontally (thus why we implemented `flip`).\n",
    "\n",
    "Summarizing some hints for both `X.grad` and `W.grad`:\n",
    "\n",
    "`X.grad`\n",
    "- The convolution of `out_grad` and `W`, with some operations applied to those\n",
    "- `W` should be flipped over both the kernel dimensions\n",
    "- If the convolution is strided, increase the size of `out_grad` with a corresponding dilation\n",
    "- Do an example to analyze dimensions: note the shape you want for `X.grad`, and think about how you must permute/transpose the arguments and add padding to the convolution to achieve this shape \n",
    "    - This padding depends on both the kernel size and the `padding` argument to the convolution\n",
    "\n",
    "`W.grad`\n",
    "- The convolution of `X` and `out_grad`, with some operations applied to those\n",
    "- The gradients of `W` must be accumulated over the batches; how can you make the conv operator itself do this accumulation?\n",
    "    - Consider turning batches into channels via transpose/permute\n",
    "- Analyze dimensions: how can you modify `X` and `out_grad` so that the shape of their convolution matches the shape of `W`? You may need to transpose/permute the result.\n",
    "    - Remember to account for the `padding` argument passed to convolution\n",
    "\n",
    "General tips\n",
    "- Deal with strided convolutions last (you should be able to just drop in `dilate` when you've passed most of the tests)\n",
    "- Start with the case where `padding=0`, then consider changing `padding` arguments\n",
    "- You can \"permute\" axes with multiple calls to `transpose`\n",
    "\n",
    "It might also be useful to skip ahead to nn.Conv, pass the forward tests, and then use both the tests below and the nn.Conv backward tests to debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fixing init._calculate_fans for convolution\n",
    "Previously, we have implemented Kaiming uniform/normal initializations, where we essentially assigned `fan_in = input_size` and `fan_out = output_size`.\n",
    "For convolution, this becomes somewhat more detailed, in that you should multiply both of these by the \"receptive field size\", which is in this case just the product of the kernel sizes -- which in our case are always going to be the same, i.e., $k\\times k$ kernels.\n",
    "\n",
    "**You will need to edit your `kaiming_uniform` in `python/needle/init/init_initializers.py`, etc. init functions to support multidimensional arrays.** In particular, it should support a new `shape` argument which is then passed to, e.g., the underlying `rand` function. Specifically, if the argument `shape` is not None, then ignore `fan_in` and `fan_out` but use the value of `shape` for initializations.\n",
    "\n",
    "You can test this below; though it is not _directly_ graded, it must match ours to pass the nn.Conv mugrade tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"kaiming_uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing nn.Conv\n",
    "\n",
    "Essentially, nn.Conv is just a wrapper of the convolution operator we previously implemented\n",
    "which adds a bias term, initializes the weight and bias, and ensures that the padding is set so that the input and output dimensions are the same (in the `stride=1` case, anyways). \n",
    "\n",
    "Importantly, nn.Conv should support NCHW format instead of NHWC format. In particular, we think this makes more sense given our current BatchNorm implementation. You can implement this by applying `transpose` twice to both the input and output.  \n",
    "\n",
    "- Ensure nn.Conv works for (N, C, H, W) tensors even though we implemented the conv op for (N, H, W, C) tensors\n",
    "- Initialize the (k, k, i, o) weight tensor using Kaiming uniform initialization with default settings\n",
    "- Initialize the (o,) bias tensor using uniform initialization on the interval $\\pm$`1.0/(in_channels * kernel_size**2)**0.5`\n",
    "- Calculate the appropriate padding to ensure input and output dimensions are the same\n",
    "- Calculate the convolution, then add the properly-broadcasted bias term if present\n",
    "\n",
    "You can now test your nn.Conv against PyTorch's nn.Conv2d with the two PyTest calls below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit nn.Conv to mugrade [20 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementing \"ResNet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use your convolutional layer to implement a model similar to _ResNet9_, which is known to be a reasonable model for getting good accuracy on CIFAR-10 quickly (see [here](https://github.com/davidcpage/cifar10-fast)). Our main change is that we used striding instead of pooling and divided all of the channels by 4 for the sake of performance (as our framework is not as well-optimized as industry-grade frameworks).\n",
    "\n",
    "In the figure below, before the linear layer, you should \"flatten\" the tensor. You can use the module `Flatten` in `nn_basic.py`, or you can simply use `.reshape` in the `forward()` method of your ResNet9.\n",
    "\n",
    "Make sure that you pass the device to all modules in your model; otherwise, you will get errors about mismatched devices when trying to run with CUDA.\n",
    "\n",
    "<center><img src=\"https://github.com/dlsyscourse/hw4/blob/main/ResNet9.png?raw=true\" alt=\"ResNet9\" style=\"width: 400px;\" /></center>\n",
    "\n",
    "We have tried to make it easier to pass the tests here than for previous assignments where you have implemented models. In particular, we are just going to make sure it has the right number of parameters and similar accuracy and loss after 1 or 2 batches of CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a ResNet on CIFAR10: (remember to copy the solutions in `python/needle/optim.py` from previous homeworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"train_cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit ResNet9 to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your model on CIFAR-10 using the following code. Note that this is likely going to be quite slow, and also  not all that accurate due to the lack of data augmentation. You should expect it to take around 500s per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "import needle as ndl\n",
    "from models import ResNet9\n",
    "from simple_ml import train_cifar10, evaluate_cifar10\n",
    "\n",
    "device = ndl.cpu()\n",
    "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
    "dataloader = ndl.data.DataLoader(\\\n",
    "         dataset=dataset,\n",
    "         batch_size=128,\n",
    "         shuffle=True,)\n",
    "model = ResNet9(device=device, dtype=\"float32\")\n",
    "train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n",
    "      lr=0.001, weight_decay=0.001)\n",
    "evaluate_cifar10(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Recurrent neural network [10 points]\n",
    "\n",
    "**Note:** In the following sections, you may find yourself wanting to index into tensors, i.e., to use getitem or setitem. However, we have not implemented these for tensors in our library; instead, you should use `stack` and `split` operations.\n",
    "\n",
    "In `python/needle/nn_sequence.py`, implement `RNNCell`.\n",
    "\n",
    "$h^\\prime = \\text{tanh}(xW_{ih} + b_{ih} + hW_{hh} + b_{hh})$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "In `python/needle/nn_sequence.py`, implement `RNN`.\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "$h_t = \\text{tanh}(x_tW_{ih} + b_{ih} + h_{(t-1)}W_{hh} + b_{hh})$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, and $h_{(t-1)}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "In a multi-layer RNN, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"test_rnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"rnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Long short-term memory network [10 points]\n",
    "In `python/needle/nn/nn_sequence.py`, implement `Sigmoid`.\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + \\text{exp}(-x)}$\n",
    "\n",
    "In `python/needle/nn/nn_sequence.py`, implement `LSTMCell`.\n",
    "\n",
    "\\begin{align}\n",
    "i &= \\sigma(xW_{ii} + b_{ii} + hW_{hi} + b_{hi}) \\\\\n",
    "f &= \\sigma(xW_{if} + b_{if} + hW_{hf} + b_{hf}) \\\\\n",
    "g &= \\text{tanh}(xW_{ig} + b_{ig} + hW_{hg} + b_{hg}) \\\\\n",
    "o &= \\sigma(xW_{io} + b_{io} + hW_{ho} + b_{ho}) \\\\\n",
    "c^\\prime &= f * c + i * g \\\\\n",
    "h^\\prime &= o * \\text{tanh}(c^\\prime)\n",
    "\\end{align}\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, and $i$, $f$, $g$, $o$ are the input, forget, cell, and output gates, respectively. \n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "Now implement `LSTM` in `python/needle/nn/nn_sequence.py`, which applies a multi-layer LSTM RNN to an input sequence. For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "\\begin{align}\n",
    "i_t &= \\sigma(x_tW_{ii} + b_{ii} + h_{(t-1)}W_{hi} + b_{hi}) \\\\\n",
    "f_t &= \\sigma(x_tW_{if} + b_{if} + h_{(t-1)}W_{hf} + b_{hf}) \\\\\n",
    "g_t &= \\text{tanh}(x_tW_{ig} + b_{ig} + h_{(t-1)}W_{hg} + b_{hg}) \\\\\n",
    "o_t &= \\sigma(x_tW_{io} + b_{io} + h_{(t-1)}W_{ho} + b_{ho}) \\\\\n",
    "c_t &= f * c_{(t-1)} + i * g \\\\\n",
    "h_t &= o * \\text{tanh}(c_t)\n",
    "\\end{align},\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is the cell state at time $t$, $x_t$ is the input at time $t$, $h_{(t-1)}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$ are the input, forget, cell, and output gates at time $t$ respectively. \n",
    "\n",
    "In a multi-layer LSTM, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"test_lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"lstm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Penn Treebank dataset [10 points]\n",
    "\n",
    "In word-level language modeling tasks, the model predicts the probability of the next word in the sequence, based on the words already observed in the sequence. You will write support for the Penn Treebank dataset, which consists of stories from the Wall Street Journal, to train and evaluate a language model on word-level prediction.\n",
    "\n",
    "In `python/needle/data/datasets/ptb_dataset.py`, start by implementing the `Dictionary` class, which creates a dictionary from a list of words, mapping each word to a unique integer.\n",
    "\n",
    "Next, we will use this `Dictionary` class to create a corpus from the train and test txt files in the Penn Treebank dataset that you downloaded at the beginning of the notebook. Implement the `tokenize` function in the `Corpus` class to do this.\n",
    "\n",
    "In order to prepare the data for training and evaluation, you will next implement the `batchify` function. Starting from sequential data, batchify arranges the dataset into columns. For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "\n",
    "```\n",
    "┌ a g m s ┐\n",
    "│ b h n t │\n",
    "│ c i o u │\n",
    "│ d j p v │\n",
    "│ e k q w │\n",
    "└ f l r x ┘\n",
    "```\n",
    "\n",
    "These columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' cannot be learned, but allows more efficient batch processing.\n",
    "\n",
    "Next, implement the `get_batch` function. `get_batch` subdivides the source data into chunks of length `bptt`. If source is equal to the example output of the batchify function, with a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "```\n",
    "┌ a g m s ┐ ┌ b h n t ┐\n",
    "└ b h n t ┘ └ c i o u ┘\n",
    "```\n",
    "Note that despite the name of the function, the subdivison of data is not done along the batch dimension (i.e. dimension 1), since that was handled by the batchify function. The chunks are along dimension 0, corresponding to the seq_len dimension in the LSTM or RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"ptb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ptb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Training a word-level language model [10 points]\n",
    "\n",
    "Finally, you will use the `RNN` and `LSTM` components you have written to construct a language model that we will train on the Penn Treebank dataset.\n",
    "\n",
    "First, in `python/needle/nn/nn_sequence.py` implement `Embedding`. Consider we have a dictionary with 1000 words. Then for a word which indexes into this dictionary, we can represent this word as a one-hot vector of size 1000, and then use a linear layer to project this to a vector of some embedding size.\n",
    "\n",
    "In `apps/models.py`, you can now implement `LanguageModel`. Your language model should consist of \n",
    "\n",
    "- An embedding layer (which maps word IDs to embeddings) \n",
    "- A sequence model (either RNN or LSTM)\n",
    "- A linear layer (which outputs probabilities of the next word)\n",
    "\n",
    "In `apps/simple_ml.py` implement `epoch_general_ptb`, `train_ptb`, and `evaluate_ptb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_implementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"language_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your language model on the Penn Treebank dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import needle as ndl\n",
    "sys.path.append('./apps')\n",
    "from models import LanguageModel\n",
    "from simple_ml import train_ptb, evaluate_ptb\n",
    "\n",
    "device = ndl.cpu()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\")\n",
    "train_data = ndl.data.batchify(corpus.train, batch_size=16, device=ndl.cpu(), dtype=\"float32\")\n",
    "model = LanguageModel(30, len(corpus.dictionary), hidden_size=10, num_layers=2, seq_model='rnn', device=ndl.cpu())\n",
    "train_ptb(model, train_data, seq_len=1, n_epochs=1, device=device)\n",
    "evaluate_ptb(model, train_data, seq_len=40, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
